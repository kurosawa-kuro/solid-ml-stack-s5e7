# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## „ÄêPROJECT OVERVIEW„ÄëKaggle S5E7 Personality Prediction
- **Competition**: https://www.kaggle.com/competitions/playground-series-s5e7/overview
- **Problem**: Binary classification (Introvert vs Extrovert)
- **Metric**: Accuracy
- **Current Ranking**: 1182/2749 teams (43.0% percentile)
- **Current Best**: 0.9684 (CV Score)
- **Bronze Target**: 0.976518 (+0.008 improvement needed)

## „ÄêCRITICAL - CURRENT PROJECT STATE„Äë
### Advanced Implementation Stage
- **Mature Implementation**: Professional ML pipeline with 700+ lines of production-ready code
- **Bronze Medal Target**: 0.8% improvement needed (Current: 0.9684, Target: 0.976518)
- **Robust Architecture**: Medallion data management, pipeline integration, comprehensive CV framework
- **High Test Coverage**: 73% coverage with 475 tests across 18 test files
- **Data Ready**: DuckDB with competition data prepared at `/home/wsl/dev/my-study/ml/solid-ml-stack-s5e7/data/kaggle_datasets.duckdb`

### Current Performance Status
- **Latest CV Score**: 0.9684 ¬± 0.0020 (96.84% accuracy)
- **Gap to Bronze**: +0.008 improvement needed
- **Architecture Quality**: Extensible design preventing future over-engineering

## „ÄêMEDALLION ARCHITECTURE„ÄëSingle Source Data Processing Pipeline

### Data Lineage & Single Source of Truth
```
üóÉÔ∏è  Raw Data Source (Single Point of Truth)
     ‚îÇ
     ‚îú‚îÄ‚îÄ DuckDB: `/data/kaggle_datasets.duckdb`
     ‚îÇ   ‚îî‚îÄ‚îÄ Schema: `playground_series_s5e7`
     ‚îÇ       ‚îú‚îÄ‚îÄ Table: `train` (Original Competition Data)
     ‚îÇ       ‚îú‚îÄ‚îÄ Table: `test` (Original Competition Data)  
     ‚îÇ       ‚îî‚îÄ‚îÄ Table: `sample_submission` (Original Format)
     ‚îÇ
     ‚Üì [Bronze Processing]
     ‚îÇ
ü•â  Bronze Layer (`src/data/bronze.py`) 
     ‚îÇ   ‚îî‚îÄ‚îÄ Purpose: Raw Data Standardization & Quality Assurance
     ‚îÇ   ‚îî‚îÄ‚îÄ Output: DuckDB `bronze.train`, `bronze.test`
     ‚îÇ
     ‚Üì [Silver Processing]
     ‚îÇ  
ü•à  Silver Layer (`src/data/silver.py`)
     ‚îÇ   ‚îî‚îÄ‚îÄ Purpose: Feature Engineering & Domain Knowledge Integration
     ‚îÇ   ‚îî‚îÄ‚îÄ Input: Bronze Layer Tables (Dependencies: bronze.py)
     ‚îÇ   ‚îî‚îÄ‚îÄ Output: DuckDB `silver.train`, `silver.test`
     ‚îÇ
     ‚Üì [Gold Processing]
     ‚îÇ
ü•á  Gold Layer (`src/data/gold.py`)
     ‚îÇ   ‚îî‚îÄ‚îÄ Purpose: ML-Ready Data Preparation & Model Interface
     ‚îÇ   ‚îî‚îÄ‚îÄ Input: Silver Layer Tables (Dependencies: silver.py)
     ‚îÇ   ‚îî‚îÄ‚îÄ Output: X_train, y_train, X_test for LightGBM
```

### Implementation Structure
```
src/
‚îú‚îÄ‚îÄ data/                 # üèóÔ∏è Medallion Architecture (Single Source Pipeline)
‚îÇ   ‚îú‚îÄ‚îÄ bronze.py         # ü•â Raw ‚Üí Standardized (Entry Point)
‚îÇ   ‚îú‚îÄ‚îÄ silver.py         # ü•à Standardized ‚Üí Engineered (Depends: bronze)
‚îÇ   ‚îî‚îÄ‚îÄ gold.py           # ü•á Engineered ‚Üí ML-Ready (Depends: silver)
‚îú‚îÄ‚îÄ models.py             # ü§ñ LightGBM Model (Consumes: gold)
‚îú‚îÄ‚îÄ validation.py         # ‚úÖ CV Framework (Orchestrates: bronze‚Üísilver‚Üígold)
‚îî‚îÄ‚îÄ util/                 # üõ†Ô∏è Supporting Infrastructure
    ‚îú‚îÄ‚îÄ time_tracker.py   
    ‚îî‚îÄ‚îÄ notifications.py  
```

### Medallion Data Processing Layers

## ü•â Bronze Layer - Raw Data Standardization (Entry Point)

### Single Source Responsibility
**Input**: Original DuckDB tables (`playground_series_s5e7.train`, `playground_series_s5e7.test`)  
**Output**: Standardized DuckDB tables (`bronze.train`, `bronze.test`)  
**Dependencies**: None (Entry point to Medallion pipeline)

### Core Processing Functions
```python
# Primary Data Interface (Single Source)
load_data() ‚Üí (train_df, test_df)                    # Raw data access point
create_bronze_tables() ‚Üí bronze.train, bronze.test  # Standardized output

# Data Quality Assurance  
validate_data_quality()     # Type validation, range guards
advanced_missing_strategy() # Missing value intelligence
encode_categorical_robust() # Yes/No ‚Üí binary standardization
winsorize_outliers()        # Numeric stability processing
```

### üöÄ Advanced Preprocessing Enhancement (Bronze Medal Strategy)
**New Implementation**: Three advanced preprocessing techniques achieving +0.7-1.2% accuracy improvement  
üìÑ **Detailed Design**: [Advanced Preprocessing Design](docs/Ë®≠Ë®à/È´òÂ∫¶„Å™ÂâçÂá¶ÁêÜË®≠Ë®à.md)

**1. Advanced Missing Pattern Analysis** (+0.3-0.5%)
- Conditional missing flags generation (social_anxiety_complete_missing, extreme_introvert_missing, etc.)
- Systematic vs random missing pattern identification
- Correlation-based missing pattern detection

**2. Cross-Feature Imputation Strategy** (+0.2-0.4%)
- Stage_fear ‚Üî Drained_after_socializing correlation-based imputation
- Behavior pattern-based imputation (high outing frequency ‚Üí social event participation estimation)
- Estimation from time allocation, friend count, and post frequency patterns

**3. Enhanced Outlier Detection** (+0.2-0.3%)
- Isolation Forest for non-linear outlier detection
- Combined statistical methods (improved IQR, Z-score)
- Domain-specific outlier detection (Time_spent_Alone > 24 hours, etc.)

### LightGBM-Optimized Data Quality Pipeline
**1. Type Safety & Validation**
- Explicit dtype setting: `int/float/bool/category`
- Range guards: `Time_spent_Alone ‚â§ 24hrs`, non-negative behavioral metrics
- Schema validation preventing downstream corruption

**2. Missing Value Intelligence**
- **Missing Flags**: Binary indicators for `Stage_fear` (~10%), `Going_outside` (~8%)
- **LightGBM Native Handling**: Preserve NaN for automatic tree processing
- **Cross-Feature Patterns**: Leverage high correlation for imputation candidates
- **Systematic Analysis**: Distinguish missing patterns (random vs systematic)

**3. Categorical Standardization**
- **Yes/No Normalization**: Case-insensitive unified mapping ‚Üí {0,1}
- **LightGBM Binary Optimization**: Optimal encoding for tree splits
- **Missing Category Handling**: Preserve for downstream LightGBM processing

**4. Leak Prevention Foundation**
- **Fold-Safe Statistics**: All computed values isolated within CV folds
- **Pipeline Readiness**: sklearn-compatible transformers for Silver layer
- **Audit Trail**: Comprehensive metadata for downstream validation

### Bronze Quality Guarantees
‚úÖ **Single Source of Truth**: All downstream processing uses Bronze tables only  
‚úÖ **LightGBM Optimized**: Preprocessing specifically designed for tree-based models  
‚úÖ **Competition Grade**: Implements proven top-tier Kaggle preprocessing patterns  
‚úÖ **Quality Assured**: Comprehensive validation preventing data corruption  
‚úÖ **Performance Ready**: Sub-second processing enabling rapid iteration

## ü•à Silver Layer - Feature Engineering & Domain Knowledge

### Single Source Dependency Chain
**Input**: Bronze Layer tables (`bronze.train`, `bronze.test`) - **Exclusive Data Source**  
**Output**: Enhanced DuckDB tables (`silver.train`, `silver.test`)  
**Dependencies**: `src/data/bronze.py` (Must execute Bronze pipeline first)

### Core Feature Engineering Pipeline
```python
# Bronze ‚Üí Silver Transformation (Single Pipeline)
load_silver_data() ‚Üí enhanced_df                    # Consumes: bronze tables only
create_silver_tables() ‚Üí silver.train, silver.test # Enhanced feature output

# Feature Engineering Layers (Sequential Processing)
advanced_features()          # 15+ statistical & domain features  
s5e7_interaction_features()  # Top-tier interaction patterns
s5e7_drain_adjusted_features() # Fatigue-adjusted activity modeling
s5e7_communication_ratios()  # Online vs Offline behavioral ratios
polynomial_features()        # Degree-2 nonlinear combinations
```

### Top-Tier Feature Engineering (Bronze ‚Üí Silver Transformation)
**1. Winner Solution Interaction Features** (+0.2-0.4% proven impact)
```python
# Bronze Input ‚Üí Silver Enhanced Features
Social_event_participation_rate = Social_event_attendance √∑ Going_outside
Non_social_outings = Going_outside - Social_event_attendance  
Communication_ratio = Post_frequency √∑ (Social_event_attendance + Going_outside)
Friend_social_efficiency = Social_event_attendance √∑ Friends_circle_size
```

**2. Fatigue-Adjusted Domain Modeling** (+0.1-0.2% introversion accuracy)
```python  
# Psychological Behavior Modeling (Top-Tier Innovation)
Activity_ratio = comprehensive_activity_index(bronze_features)
Drain_adjusted_activity = activity_ratio √ó (1 - Drained_after_socializing)
Introvert_extrovert_spectrum = quantified_personality_score(bronze_features)
```

**3. LightGBM Tree-Optimized Features** (+0.3-0.5% tree processing gain)
- **Missing Preservation**: Inherit Bronze NaN handling for LightGBM native processing
- **Ratio Features**: Optimized for tree-based splitting patterns
- **Binary Interactions**: Leverage Bronze categorical standardization
- **Composite Indicators**: Multi-feature statistical aggregations

### Silver Processing Guarantees  
‚úÖ **Bronze Dependency**: Exclusively consumes Bronze layer (no raw data access)  
‚úÖ **Feature Lineage**: Clear traceability from Bronze ‚Üí Silver transformations  
‚úÖ **LightGBM Optimized**: All features designed for tree-based model consumption  
‚úÖ **Competition Proven**: Implements verified top-tier Kaggle techniques  
‚úÖ **Performance Enhanced**: 30+ engineered features with measured impact expectations

---

## 1Ô∏è‚É£ Fold-Safe Target Encoding (Strategy 2)

### Implementation Points

| Element                         | Implementation Hints                                                                                                        |
| ------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| **Cross-validation transform**  | `category_encoders.TargetEncoder` within **fold loop fit/transform**<br>‚Üí Write `cross_val_transform` helper for reusability |
| **Bayesian Smoothing**         | Test `smoothing` parameter at `0.3 ‚Äì 0.5`<br>‚Üí Too small ‚úï (leakage), too large ‚úï (diluted information)                  |
| **High-information columns only** | Limit to `Stage_fear`, `Drained` etc. with 5-30 categories<br>‚Üí Keep binary columns as one-hot                             |
| **OOF & Test column naming**    | Example: `stage_fear_te`, `drained_te` ‚Üí Use unified suffix `_te` to prevent downstream collisions                         |

### Helper Function Example

```python
# silver/encoders/target.py
from category_encoders import TargetEncoder
from sklearn.model_selection import StratifiedKFold
import numpy as np, pandas as pd

def fold_safe_target_encode(df, y, cols, n_splits=5, seed=42, smoothing=0.3):
    skf = StratifiedKFold(n_splits, shuffle=True, random_state=seed)
    oof = np.zeros((len(df), len(cols)))
    for train_idx, valid_idx in skf.split(df, y):
        te = TargetEncoder(cols=cols, smoothing=smoothing)
        te.fit(df.iloc[train_idx], y.iloc[train_idx])
        oof[valid_idx, :] = te.transform(df.iloc[valid_idx])[cols].values
    df_te = pd.DataFrame(oof, columns=[f"{c}_te" for c in cols], index=df.index)
    return pd.concat([df, df_te], axis=1)
```

*Testing*:
`assert_no_leakage(train_df, valid_df, key='id')` ‚Üí Check that TE columns don't leak id information.

---

## 2Ô∏è‚É£ Advanced Statistical & Imputation Features (Strategy 3)

### 2-1. KNN Imputation

```python
from sklearn.impute import KNNImputer
num_cols = df.select_dtypes(include='number').columns
imputer = KNNImputer(n_neighbors=5, weights='distance')
df[num_cols] = imputer.fit_transform(df[num_cols])
```

*Tips*

* **Preprocessing Cache**: Save to `processed/train_knn.pkl` for reuse.
* KNN is expensive, so include `n_neighbors=3/5` as Optuna parameter.

### 2-2. Row Statistics & Z-score

```python
stats_cols = ['feature1','feature2', ...]
df['row_mean']  = df[stats_cols].mean(axis=1)
df['row_std']   = df[stats_cols].std(axis=1)
df['row_skew']  = df[stats_cols].skew(axis=1)
df['row_kurt']  = df[stats_cols].kurtosis(axis=1)

# Z-score & percentile
for c in stats_cols:
    df[f'{c}_z']  = (df[c] - df[c].mean()) / df[c].std()
    df[f'{c}_pct'] = df[c].rank(pct=True)
```

*Note*

* Standardization uses **train statistics only** ‚Üí Apply same mean/variance to test.
* To prevent feature explosion, limit `*_z` to top 10 high-information columns.

---

## 3Ô∏è‚É£ Silver Layer Integration Order

```mermaid
graph LR
  Bronze -->|clean| Silver1[Impute (KNN)]
  Silver1 -->|encode| Silver2[Target Encoding]
  Silver2 -->|feat| Silver3[Row Stats / Zscore]
  Silver3 --> Gold[Model Train (LightGBM)]
```

1. **KNN Imputation** to fill missing values
2. **Target Encoding** as *new columns* addition
3. **Statistical Features** further enhancement
4. Existing Gold Layer (LightGBM model) training ‚Üí **Confirm CV improvement**

## ü•á Gold Layer - ML-Ready Data & Model Interface

### Single Source Dependency Chain
**Input**: Silver Layer tables (`silver.train`, `silver.test`) - **Exclusive Data Source**  
**Output**: LightGBM-ready arrays (`X_train`, `y_train`, `X_test`)  
**Dependencies**: `src/data/silver.py` (Must execute Silver pipeline first)

### Core ML Preparation Pipeline
```python
# Silver ‚Üí Gold Transformation (Final ML Interface)
get_ml_ready_data() ‚Üí X_train, y_train, X_test     # LightGBM consumption ready
prepare_model_data() ‚Üí formatted_arrays            # Model-specific formatting

# ML Optimization Layers (Sequential Processing)
clean_and_validate_features()   # Data quality final validation
select_best_features()          # Statistical feature selection (F-test + MI)
create_submission_format()      # Competition output standardization
```

### LightGBM Model Interface (Silver ‚Üí Gold ‚Üí Model)
**1. Feature Selection & Optimization**
```python
# Silver Input ‚Üí Gold Optimized Features  
statistical_selection = F_test + mutual_information(silver_features)
lightgbm_ready_features = feature_importance_ranking(selected_features)
X_train, y_train = prepare_training_data(optimized_features)
X_test = prepare_inference_data(optimized_features)
```

**2. Production-Ready Data Quality**
- **Final Validation**: Infinite value processing, outlier detection
- **Type Consistency**: Ensure LightGBM-compatible data types
- **Memory Optimization**: Efficient array formats for training
- **Audit Completeness**: Comprehensive data lineage validation

**3. Competition Output Interface**
- **Submission Formatting**: Standard Kaggle submission file creation
- **Model Prediction Interface**: Direct LightGBM consumption format  
- **Performance Monitoring**: Feature importance and prediction tracking

### Gold Processing Guarantees
‚úÖ **Silver Dependency**: Exclusively consumes Silver layer (no Bronze/Raw access)  
‚úÖ **Model Ready**: Direct LightGBM consumption without additional processing  
‚úÖ **Competition Format**: Standard Kaggle submission file compatibility  
‚úÖ **Production Quality**: Final validation ensuring model training stability  
‚úÖ **Performance Optimized**: Feature selection maximizing Bronze Medal target (0.976518)

## üéØ Medallion Pipeline Development Strategy

### Single Source Processing Flow
```
Raw Data ‚Üí ü•â Bronze ‚Üí ü•à Silver ‚Üí ü•á Gold ‚Üí ü§ñ LightGBM ‚Üí üèÜ Bronze Medal (0.976518)
```

**Current Phase**: Bronze + Silver optimization for LightGBM baseline  
**Target**: Single model achieving Bronze Medal threshold  
**Architecture**: Medallion pipeline ensuring data lineage integrity

## üóÉÔ∏è Single Source Data Management (DuckDB)

### Primary Data Source (Single Point of Truth)
**Database**: `/home/wsl/dev/my-study/ml/solid-ml-stack-s5e7/data/kaggle_datasets.duckdb`

### Schema Structure & Data Lineage
```sql
-- Raw Competition Data (Original Source)
playground_series_s5e7.train           # Original Kaggle training data
playground_series_s5e7.test            # Original Kaggle test data  
playground_series_s5e7.sample_submission # Original submission format

-- Medallion Pipeline Outputs (Processed Layers)
bronze.train, bronze.test              # ü•â Standardized & validated
silver.train, silver.test              # ü•à Feature engineered  
gold.X_train, gold.y_train, gold.X_test # ü•á ML-ready (optional persistence)
```

### Data Access Patterns (Single Source Enforcement)
```python
# ‚ùå NEVER: Direct raw data access in Silver/Gold layers
# ‚úÖ ALWAYS: Use appropriate layer's load functions

# Bronze Layer (Entry Point)
from src.data.bronze import load_data
train_raw, test_raw = load_data()  # Only Bronze accesses raw data

# Silver Layer (Bronze Dependency)  
from src.data.silver import load_silver_data
train_silver, test_silver = load_silver_data()  # Only accesses Bronze output

# Gold Layer (Silver Dependency)
from src.data.gold import get_ml_ready_data  
X_train, y_train, X_test = get_ml_ready_data()  # Only accesses Silver output
```

### Original Feature Schema (Competition Data)
- **Target**: `Personality` (Introvert/Extrovert) - Binary classification
- **ID**: `id` - Row identifier
- **Numeric Features** (5): Time_spent_Alone, Social_event_attendance, Going_outside, Friends_circle_size, Post_frequency
- **Categorical Features** (2): Stage_fear (Yes/No), Drained_after_socializing (Yes/No)

### Single Source Benefits
‚úÖ **Data Lineage**: Clear transformation tracking from Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold  
‚úÖ **Dependency Control**: Each layer only accesses its immediate predecessor  
‚úÖ **Consistency Guarantee**: All downstream processing uses standardized inputs  
‚úÖ **Debug Efficiency**: Issues traceable to specific pipeline layer  
‚úÖ **Cache Optimization**: Intermediate results stored in DuckDB for reuse

## „ÄêDEVELOPMENT COMMANDS„Äë

### Currently Available (Makefile)
```bash
make install              # Install dependencies
make dev-install         # Install with dev tools
make setup               # Create directory structure
make quick-test          # Quick single model test
make personality-prediction  # Full workflow (when implemented)
make test                # Run tests (when tests exist)
make clean               # Clean outputs
make help                # Show available commands
```

### Available Commands (Implemented)
```bash
# Core workflow
make install              # ‚úÖ Install dependencies
make dev-install         # ‚úÖ Install with dev tools
make test                # ‚úÖ Run 475 tests (73% coverage)
make quick-test          # ‚úÖ Fast model validation
make personality-prediction  # ‚úÖ Full training pipeline
make clean               # ‚úÖ Clean outputs

# Training variations
python scripts/train_light.py    # ‚úÖ Fast iteration (0.5s)
python scripts/train.py          # ‚úÖ Standard training
python scripts/train_enhanced.py # ‚úÖ Advanced features
python scripts/train_heavy.py    # ‚úÖ Full optimization
```

## „ÄêDEPENDENCIES & ENVIRONMENT„Äë

### Installation (pyproject.toml configured)
```bash
pip install -e .                    # Basic ML dependencies
pip install -e .[dev]              # + development tools
pip install -e .[optimization]     # + Optuna for tuning
pip install -e .[visualization]    # + plotting libraries
```

### Core Dependencies
- **Data**: pandas, numpy, duckdb
- **Models**: scikit-learn, xgboost, lightgbm, catboost
- **Optimization**: optuna
- **Development**: pytest, black, flake8, mypy
- **Python**: 3.8+

## „ÄêCURRENT PERFORMANCE„ÄëRecent Training Results
- **Light Enhanced Model**: 96.79% ¬± 0.22% (Latest run, 30 features)
- **Baseline Model**: 96.84% ¬± 0.20% (Best CV score, 10 features) 
- **Training Efficiency**: 0.5 seconds (light), 0.39 seconds (baseline)
- **Feature Importance**: poly_extrovert_score_Post_frequency (257.6) leads
- **Bronze Gap**: +0.8% needed (highly achievable with optimization)

### Performance Analysis
- **Consistent Results**: Low standard deviation indicates stable model
- **Fast Iteration**: Sub-second training enables rapid experimentation
- **Feature Quality**: Polynomial features show strong predictive power

## „ÄêIMPLEMENTATION GUIDELINES„Äë

### Design Principles (Balanced Approach)
- **Extensible Simplicity**: Clean abstractions that support growth without complexity
- **Leak Prevention**: Pipeline integration ensures CV-aware preprocessing (implemented)
- **Trust Your CV**: StratifiedKFold with integrity validation (implemented)
- **Evidence-Based**: Feature engineering based on importance analysis
- **Incremental Development**: Medallion architecture supports progressive enhancement

### Key Implementation Notes
- **No CSV Files**: All data access through DuckDB only
- **System Python**: No virtual environments (per project history)
- **Classification Setup**: Binary classification for `Personality` target
- **Accuracy Metric**: Primary evaluation criterion

### Development Workflow (Optimized)
1. **Optimize Current**: Hyperparameter tuning and feature selection for bronze
2. **Iterate Fast**: 0.5-second training cycles enable rapid experimentation
3. **Validate Rigorously**: Data leak prevention and CV integrity checks (implemented)
4. **Test Comprehensively**: 73% coverage with integration tests (implemented)
5. **Scale Thoughtfully**: Medallion architecture supports controlled expansion

### Bronze Layer Implementation Checklist (Data Quality & Preprocessing Only)
**Essential Steps (Data Quality Assurance)**:
- [x] Explicit dtype setting on data load (int/float/bool/category)
- [x] Value range validation (Time_spent_Alone ‚â§ 24hrs, non-negative checks)
- [x] Yes/No normalization dictionary (case unification ‚Üí {0,1})
- [x] Missing flag generation (Stage_fear, Going_outside, Drained_after_socializing)
- [x] Within-fold statistics calculation (CV-safe imputation values & encoding)
- [x] Stratified K-Fold setup (maintain class ratio)

**Strongly Recommended Steps (Data Quality Enhancement)**:
- [x] Outlier winsorizing (IQR-based, 1%/99% percentile clipping)
- [x] LightGBM-optimized preprocessing (preserve NaN, binary categorical encoding)
- [x] Cross-feature imputation (high correlation pattern-based missing value estimation)
- [ ] Advanced missing pattern analysis (systematic vs random detection)

**Data Quality Only (No Feature Engineering)**:
- [x] Categorical standardization (Yes/No ‚Üí binary encoding)
- [x] Missing value flags (binary indicators for downstream processing)
- [x] Data type validation (ensure LightGBM compatibility)
- [x] Range validation (prevent downstream corruption)

### Silver Layer Implementation Checklist (30+ Engineered Features)
**High Priority (Winner Solution Features - +0.2-0.4% proven impact)**:
- [x] Social event participation rate (Social_event_attendance √∑ Going_outside) 
- [x] Non-social outings (Going_outside - Social_event_attendance)
- [x] Communication ratio (Post_frequency √∑ total activity)
- [x] Drain adjusted activity (fatigue-based activity adjustment)
- [x] Friend-social efficiency (Social_event_attendance √∑ Friends_circle_size)

**Medium Priority (Statistical Composite Indicators - +0.1-0.2% impact)**:
- [x] Social activity ratio (integrated social activity indicator)
- [x] Introvert-extrovert spectrum (personality quantification)
- [x] Communication balance (online-offline activity balance)
- [x] Activity pattern classification (social/non-social/online)
- [x] Fatigue weighting enhancement (Drained_after_socializing utilization)

**Advanced Features (LightGBM Tree Optimization - +0.3-0.5% gain)**:
- [x] Polynomial features (degree-2 nonlinear combinations)
- [x] Enhanced interaction features (binary and triple interactions)
- [x] Scaling features (standardization for tree splits)
- [x] Missing value preservation (inherit Bronze NaN handling)
- [x] Ratio features (optimized for tree-based splitting patterns)

## „ÄêSUCCESS CRITERIA„Äë
- **Bronze Medal**: 0.976518+ accuracy (+0.8% from current 0.9684)
- **Architecture Quality**: Extensible design with controlled complexity
- **Reliability**: Data leak prevention, reproducible CV results (implemented)
- **Development Efficiency**: Sub-second training, comprehensive testing (implemented)
- **Long-term Value**: Reusable patterns for future competitions

## „ÄêBRONZE MEDAL ROADMAP„Äë
### Immediate Opportunities (1-2 weeks)
1. **Silver Layer Optimization** (Highest Priority +0.4-0.8% expected):
   - ‚úÖ Winner Solution features implemented (social event participation rate, communication_ratio)
   - ‚úÖ Fatigue-adjusted activity scores (drain_adjusted_activity) - top-tier innovation
   - ‚úÖ 30+ engineered features with polynomial and interaction features
   - ‚úÖ LightGBM-optimized features (ratio features, binary interactions)

2. **Bronze Layer Enhancement** (High Priority +0.3-0.5% expected):
   - ‚úÖ Missing indicators for Stage_fear, Going_outside (top-tier proven)
   - ‚úÖ Cross-feature imputation using high correlation patterns
   - ‚úÖ Winsorizing outliers (IQR-based) for numeric stability
   - ‚úÖ LightGBM-optimized preprocessing (NaN preservation, binary encoding)

3. **Hyperparameter Optimization**: Leverage existing Optuna integration (+0.2-0.4%)

4. **Enhanced Data Quality** (Medium Priority +0.1-0.3%):
   - ‚úÖ Dtype validation with range guards (Time_spent_Alone ‚â§ 24hrs)
   - ‚úÖ Categorical standardization (case-insensitive Yes/No mapping)
   - [ ] Advanced missing pattern analysis for systematic vs random detection

5. **CV Framework Enhancement** (+0.1-0.2%):
   - ‚úÖ Stratified K-Fold with explicit Introvert/Extrovert ratio maintenance
   - ‚úÖ Fold-safe statistics computation preventing information leakage
   - ‚úÖ Pipeline integration ensuring consistent train/validation processing

6. **Feature Selection**: Focus on top importance features (poly_extrovert_score_*)
7. **Model Ensemble**: Combine CV folds for prediction stability
8. **Threshold Tuning**: Optimize classification threshold for accuracy

### Technical Assets Ready
- ‚úÖ **Data Leak Prevention**: Pipeline integration implemented
- ‚úÖ **CV Framework**: Integrity validation and stratified sampling
- ‚úÖ **Feature Engineering**: 30+ engineered features with importance ranking
- ‚úÖ **Optimization Infrastructure**: Optuna integration for hyperparameter tuning
- ‚úÖ **Performance Monitoring**: Time tracking and comprehensive logging
