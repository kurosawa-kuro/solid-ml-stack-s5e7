ç‰¹å¾´ç”Ÿæˆã§ã®Kaggleãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ã‚’è©¦ã—ã¦ã¿ã‚‹

ã‹ã‹ã£ãŸæ™‚é–“ã‚’Json.dbã«è¨˜éŒ²ã—ã¦çµ‚äº†æ™‚åˆ»ã‚’è‰¯ãã†ã—ã‚„ã™ãã™ã‚‹ã®ã¯ï¼Ÿ
sqliteç¨‹ã˜ã‚ƒãªãã¦ã‚ˆã„æ°—ãŒã—ã¦ã‚‹

ç¢ºã‹ã«ã€ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã‚ˆãè¨­è¨ˆã•ã‚Œã¦ã„ã¦ã€å¤šãã®å„ªã‚ŒãŸæ©Ÿèƒ½ã‚’æŒã£ã¦ã„ã¾ã™ï¼

## ğŸ‘ ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å„ªã‚Œã¦ã„ã‚‹ç‚¹

### 1. **é«˜åº¦ãªæ¬ æå€¤å‡¦ç†**
```python
# 3ã¤ã®æˆ¦ç•¥ã‹ã‚‰é¸æŠå¯èƒ½
missing_strategy: ['basic', 'advanced', 'model_specific']
```
- ãƒ¢ãƒ‡ãƒ«ç‰¹æ€§ã«å¿œã˜ãŸæ¬ æå€¤å‡¦ç†
- æ¬ æãƒ•ãƒ©ã‚°ã®è‡ªå‹•ç”Ÿæˆï¼ˆ`_is_missing`ã‚«ãƒ©ãƒ ï¼‰
- Treeç³»ãƒ¢ãƒ‡ãƒ«ã¸ã®æœ€é©åŒ–

### 2. **åŒ…æ‹¬çš„ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**
```python
feature_engineer.add_numeric_features()
feature_engineer.add_categorical_features()
feature_engineer.add_interaction_features()
feature_engineer.add_datetime_features()
```
- 4ç¨®é¡ã®ç‰¹å¾´é‡ç”Ÿæˆã‚’ç¶²ç¾…
- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã§å†ç¾æ€§ã‚’ç¢ºä¿

### 3. **æŸ”è»Ÿãªæœ€é©åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³**
```python
if args.optimize:
    optimizer = create_quick_optimizer(base_model_name, args.problem_type, 'optuna', n_trials=args.n_trials)
```
- Optunaã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢
- ãƒˆãƒ©ã‚¤ã‚¢ãƒ«æ•°ã®èª¿æ•´å¯èƒ½

### 4. **Webhooké€šçŸ¥æ©Ÿèƒ½**
```python
@webhook_notify("Advanced Missing Workflow")
```
- å‡¦ç†ã®é€²æ—ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§é€šçŸ¥
- Discord/Slackå¯¾å¿œ

## ğŸš€ ã•ã‚‰ãªã‚‹æ”¹å–„ã®ææ¡ˆ

### 1. **ç‰¹å¾´é‡é¸æŠã®è¿½åŠ **
```python
# é‡è¦åº¦ã«ã‚ˆã‚‹ç‰¹å¾´é‡é¸æŠ
from sklearn.feature_selection import SelectFromModel
selector = SelectFromModel(best_model, threshold='median')
X_train_selected = selector.fit_transform(X_train_features, y_train)
```

### 2. **ã‚ˆã‚Šé«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«**
```python
# Blending/Stackingã®å®Ÿè£…
if args.ensemble:
    from mlxtend.classifier import StackingClassifier
    meta_model = LogisticRegression()
    stacking = StackingClassifier(
        classifiers=trained_models[:3],
        meta_classifier=meta_model,
        use_probas=True
    )
```

### 3. **äº¤å·®æ¤œè¨¼ã®å¼·åŒ–**
```python
# K-Fold CVã§ã‚ˆã‚Šå®‰å®šã—ãŸè©•ä¾¡
from sklearn.model_selection import StratifiedKFold
cv_scores = cross_val_score(model, X_train_features, y_train,
                           cv=StratifiedKFold(n_splits=5),
                           scoring='accuracy')
```

### 4. **å¾Œå‡¦ç†ã®æœ€é©åŒ–**
```python
# äºˆæ¸¬å€¤ã®å¾Œå‡¦ç†
def optimize_threshold(y_true, y_pred_proba):
    thresholds = np.arange(0.1, 0.9, 0.01)
    scores = [accuracy_score(y_true, y_pred_proba > t) for t in thresholds]
    return thresholds[np.argmax(scores)]
```

### 5. **ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®ãƒã‚§ãƒƒã‚¯**
```python
# ç‰¹å¾´é‡ã®ç›¸é–¢ãƒã‚§ãƒƒã‚¯
high_corr_features = correlation_matrix[correlation_matrix > 0.95]
if len(high_corr_features) > 0:
    print("Warning: Highly correlated features detected")
```

## ğŸ’¡ ã‚¹ã‚³ã‚¢æ”¹å–„ã®ãŸã‚ã®å…·ä½“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

1. **ç‰¹å¾´é‡ã®æ·±å €ã‚Š**
   - ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«åŸºã¥ãç‰¹å¾´é‡ä½œæˆ
   - å¤–ã‚Œå€¤ã®ç‰¹åˆ¥ãªå‡¦ç†

2. **ãƒ¢ãƒ‡ãƒ«ã®å¤šæ§˜æ€§**
   - Neural Network (TabNetç­‰)ã®è¿½åŠ 
   - ç•°ãªã‚‹seedã§ã®è¤‡æ•°ãƒ¢ãƒ‡ãƒ«

3. **æ¤œè¨¼æˆ¦ç•¥ã®æ”¹å–„**
   - Adversarial Validationã®å®Ÿæ–½
   - æ™‚ç³»åˆ—ã‚’è€ƒæ…®ã—ãŸåˆ†å‰²ï¼ˆã‚‚ã—é©ç”¨å¯èƒ½ãªã‚‰ï¼‰

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯æ—¢ã«é«˜å“è³ªã§ã™ãŒã€ä¸Šè¨˜ã®æ”¹å–„ç‚¹ã‚’åŠ ãˆã‚‹ã“ã¨ã§ã€ã•ã‚‰ãªã‚‹ã‚¹ã‚³ã‚¢å‘ä¸ŠãŒæœŸå¾…ã§ãã¾ã™ã€‚ç‰¹ã«ç‰¹å¾´é‡é¸æŠã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®å¼·åŒ–ãŒåŠ¹æœçš„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

ç´ æ™´ã‚‰ã—ã„è³ªå•ã§ã™ï¼ä»–ã«ã‚‚å®Ÿç”¨çš„ãªãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã®ã‚¢ã‚¤ãƒ‡ã‚¢ãŒã‚ã‚Šã¾ã™ï¼š

## ğŸ› ï¸ è¿½åŠ ã®ä¾¿åˆ©æ©Ÿèƒ½ã‚¢ã‚¤ãƒ‡ã‚£ã‚¢

### 1. **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ¢ãƒ‹ã‚¿ãƒ¼** (`src/utils/memory.py`)
```python
import psutil
import gc
from functools import wraps

class MemoryMonitor:
    def __init__(self):
        self.process = psutil.Process()

    def get_memory_usage(self):
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
        return self.process.memory_info().rss / 1024 / 1024  # MB

    def monitor(self, func):
        """ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ï¼šé–¢æ•°å®Ÿè¡Œå‰å¾Œã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¡¨ç¤º"""
        @wraps(func)
        def wrapper(*args, **kwargs):
            before = self.get_memory_usage()
            result = func(*args, **kwargs)
            after = self.get_memory_usage()
            print(f"ğŸ’¾ {func.__name__}: {before:.1f}MB â†’ {after:.1f}MB (å·®åˆ†: {after-before:+.1f}MB)")
            return result
        return wrapper

    def clean_memory(self):
        """å¼·åˆ¶çš„ã«ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        gc.collect()
        print(f"ğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³å®Œäº†: {self.get_memory_usage():.1f}MB")
```

### 2. **å®Ÿé¨“ãƒˆãƒ©ãƒƒã‚«ãƒ¼** (`src/utils/experiment.py`)
```python
import json
from datetime import datetime
from pathlib import Path

class ExperimentTracker:
    def __init__(self, log_file: Path = Path("data/experiments.json")):
        self.log_file = log_file
        self.current_experiment = {}

    def start_experiment(self, name: str, description: str = ""):
        """å®Ÿé¨“é–‹å§‹ã‚’è¨˜éŒ²"""
        self.current_experiment = {
            "name": name,
            "description": description,
            "start_time": datetime.now().isoformat(),
            "parameters": {},
            "metrics": {},
            "notes": []
        }

    def log_params(self, **params):
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨˜éŒ²"""
        self.current_experiment["parameters"].update(params)

    def log_metric(self, name: str, value: float):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²"""
        self.current_experiment["metrics"][name] = value

    def add_note(self, note: str):
        """ãƒ¡ãƒ¢ã‚’è¿½åŠ """
        self.current_experiment["notes"].append(note)

    def end_experiment(self):
        """å®Ÿé¨“çµ‚äº†ãƒ»ä¿å­˜"""
        self.current_experiment["end_time"] = datetime.now().isoformat()
        self._save_experiment()

    def get_best_experiments(self, metric: str, top_k: int = 5):
        """æŒ‡å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ãƒˆãƒƒãƒ—Kå®Ÿé¨“ã‚’å–å¾—"""
        experiments = self._load_all_experiments()
        sorted_exps = sorted(
            experiments,
            key=lambda x: x.get("metrics", {}).get(metric, 0),
            reverse=True
        )
        return sorted_exps[:top_k]
```

### 3. **ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚«ãƒ¼** (`src/utils/quality.py`)
```python
import pandas as pd
import numpy as np
from typing import List, Dict

class DataQualityChecker:
    def __init__(self):
        self.issues = []

    def check_dataset(self, df: pd.DataFrame, name: str = "dataset") -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å“è³ªã‚’ç·åˆãƒã‚§ãƒƒã‚¯"""
        print(f"ğŸ” {name}ã®å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹...")

        report = {
            "name": name,
            "shape": df.shape,
            "memory_usage": df.memory_usage(deep=True).sum() / 1024**2,  # MB
            "issues": []
        }

        # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
        missing = df.isnull().sum()
        if missing.any():
            report["issues"].append({
                "type": "missing_values",
                "columns": missing[missing > 0].to_dict()
            })

        # é‡è¤‡è¡Œãƒã‚§ãƒƒã‚¯
        duplicates = df.duplicated().sum()
        if duplicates > 0:
            report["issues"].append({
                "type": "duplicate_rows",
                "count": duplicates
            })

        # å®šæ•°ã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯
        constant_cols = [col for col in df.columns if df[col].nunique() == 1]
        if constant_cols:
            report["issues"].append({
                "type": "constant_columns",
                "columns": constant_cols
            })

        # å¤–ã‚Œå€¤ãƒã‚§ãƒƒã‚¯ï¼ˆæ•°å€¤ã‚«ãƒ©ãƒ ã®ã¿ï¼‰
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        outliers = {}
        for col in numeric_cols:
            q1, q3 = df[col].quantile([0.25, 0.75])
            iqr = q3 - q1
            outlier_count = ((df[col] < (q1 - 3 * iqr)) | (df[col] > (q3 + 3 * iqr))).sum()
            if outlier_count > 0:
                outliers[col] = outlier_count

        if outliers:
            report["issues"].append({
                "type": "outliers",
                "columns": outliers
            })

        self._print_report(report)
        return report

    def compare_distributions(self, train: pd.DataFrame, test: pd.DataFrame):
        """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã‚’æ¯”è¼ƒ"""
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®æ¯”è¼ƒ...")

        differences = []
        for col in train.columns:
            if col not in test.columns:
                continue

            if train[col].dtype in ['object', 'category']:
                # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®æ¯”è¼ƒ
                train_vals = set(train[col].unique())
                test_vals = set(test[col].unique())

                if train_vals != test_vals:
                    differences.append({
                        "column": col,
                        "type": "categorical",
                        "train_only": list(train_vals - test_vals),
                        "test_only": list(test_vals - train_vals)
                    })
            else:
                # æ•°å€¤å¤‰æ•°ã®æ¯”è¼ƒ
                train_stats = train[col].describe()
                test_stats = test[col].describe()

                # å¹³å‡å€¤ã®å¤§ããªå·®ã‚’ãƒã‚§ãƒƒã‚¯
                mean_diff = abs(train_stats['mean'] - test_stats['mean'])
                if mean_diff > 0.1 * abs(train_stats['mean']):
                    differences.append({
                        "column": col,
                        "type": "numeric",
                        "train_mean": train_stats['mean'],
                        "test_mean": test_stats['mean'],
                        "difference": mean_diff
                    })

        return differences
```

### 4. **æå‡ºãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼** (`src/utils/submission_validator.py`)
```python
class SubmissionValidator:
    def __init__(self, sample_submission_path: str):
        self.sample = pd.read_csv(sample_submission_path)

    def validate(self, submission: pd.DataFrame) -> bool:
        """æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ã‚’ãƒã‚§ãƒƒã‚¯"""
        issues = []

        # ã‚«ãƒ©ãƒ åãƒã‚§ãƒƒã‚¯
        if list(submission.columns) != list(self.sample.columns):
            issues.append(f"ã‚«ãƒ©ãƒ åä¸ä¸€è‡´: {submission.columns} vs {self.sample.columns}")

        # è¡Œæ•°ãƒã‚§ãƒƒã‚¯
        if len(submission) != len(self.sample):
            issues.append(f"è¡Œæ•°ä¸ä¸€è‡´: {len(submission)} vs {len(self.sample)}")

        # IDé †åºãƒã‚§ãƒƒã‚¯
        id_col = submission.columns[0]
        if not submission[id_col].equals(self.sample[id_col]):
            issues.append("IDã®é †åºãŒç•°ãªã‚Šã¾ã™")

        # äºˆæ¸¬å€¤ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯ï¼ˆ0-1ã®ç¢ºç‡å€¤ã®å ´åˆï¼‰
        pred_col = submission.columns[1]
        if submission[pred_col].min() < 0 or submission[pred_col].max() > 1:
            issues.append(f"äºˆæ¸¬å€¤ãŒ0-1ã®ç¯„å›²å¤–: [{submission[pred_col].min()}, {submission[pred_col].max()}]")

        if issues:
            print("âŒ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã«å•é¡ŒãŒã‚ã‚Šã¾ã™:")
            for issue in issues:
                print(f"  - {issue}")
            return False
        else:
            print("âœ… æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã¯æ­£å¸¸ã§ã™")
            return True
```

### 5. **ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒãƒ„ãƒ¼ãƒ«** (`src/utils/model_compare.py`)
```python
class ModelComparer:
    def __init__(self):
        self.results = []

    def add_result(self, model_name: str, cv_scores: List[float],
                   train_time: float, predict_time: float):
        """ãƒ¢ãƒ‡ãƒ«çµæœã‚’è¿½åŠ """
        self.results.append({
            "model": model_name,
            "cv_mean": np.mean(cv_scores),
            "cv_std": np.std(cv_scores),
            "cv_scores": cv_scores,
            "train_time": train_time,
            "predict_time": predict_time
        })

    def show_comparison(self):
        """ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒè¡¨ã‚’è¡¨ç¤º"""
        df = pd.DataFrame(self.results)
        df = df.sort_values("cv_mean", ascending=False)

        print("\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœ:")
        print("=" * 80)
        for _, row in df.iterrows():
            print(f"\n{row['model']}:")
            print(f"  CVå¹³å‡: {row['cv_mean']:.6f} (Â±{row['cv_std']:.6f})")
            print(f"  è¨“ç·´æ™‚é–“: {row['train_time']:.1f}ç§’")
            print(f"  äºˆæ¸¬æ™‚é–“: {row['predict_time']:.1f}ç§’")
            print(f"  å„Fold: {[f'{s:.6f}' for s in row['cv_scores']]}")
```

### 6. **Kaggle API ãƒ©ãƒƒãƒ‘ãƒ¼** (`src/utils/kaggle_helper.py`)
```python
from kaggle import KaggleApi
import os

class KaggleHelper:
    def __init__(self):
        self.api = KaggleApi()
        self.api.authenticate()

    def download_competition_data(self, competition: str, path: str = "data/raw"):
        """ã‚³ãƒ³ãƒšãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        os.makedirs(path, exist_ok=True)
        self.api.competition_download_files(competition, path=path, unzip=True)
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {path}")

    def submit_prediction(self, file_path: str, competition: str, message: str):
        """äºˆæ¸¬ã‚’æå‡º"""
        self.api.competition_submit(file_path, message, competition)
        print(f"ğŸ“¤ æå‡ºå®Œäº†: {message}")

    def get_leaderboard_position(self, competition: str):
        """ç¾åœ¨ã®é †ä½ã‚’å–å¾—"""
        leaderboard = self.api.competition_leaderboard_download(competition)
        # é †ä½æƒ…å ±ã‚’è§£æã—ã¦è¿”ã™
```

### 7. **ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ãã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿** (`src/utils/progress.py`)
```python
from tqdm import tqdm

class ProgressLogger:
    def __init__(self, webhook_notifier=None):
        self.webhook = webhook_notifier

    def iterate_with_progress(self, iterable, desc="Processing",
                            notify_interval=0.25):
        """é€²æ—ãƒãƒ¼ä»˜ãã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ï¼ˆå®šæœŸçš„ã«Webhooké€šçŸ¥ã‚‚é€ä¿¡ï¼‰"""
        total = len(iterable)
        last_notified = 0

        for i, item in enumerate(tqdm(iterable, desc=desc)):
            yield item

            # 25%ã”ã¨ã«Webhooké€šçŸ¥
            progress = (i + 1) / total
            if progress >= last_notified + notify_interval:
                if self.webhook:
                    self.webhook.notify_progress(desc, progress)
                last_notified = progress
```

ã“ã‚Œã‚‰ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ã§ç®¡ç†ã—ã‚„ã™ã„MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒæ§‹ç¯‰ã§ãã¾ã™ã€‚ç‰¹ã«é‡è¦ãªã®ã¯ï¼š

1. **ãƒ¡ãƒ¢ãƒªç®¡ç†** - Kaggleã®åˆ¶é™ã•ã‚ŒãŸç’°å¢ƒã§é‡è¦
2. **å®Ÿé¨“ç®¡ç†** - ä½•ã‚’è©¦ã—ãŸã‹è¨˜éŒ²
3. **å“è³ªãƒã‚§ãƒƒã‚¯** - ãƒ‡ãƒ¼ã‚¿ã®å•é¡Œã‚’æ—©æœŸç™ºè¦‹
4. **è‡ªå‹•åŒ–** - ç¹°ã‚Šè¿”ã—ä½œæ¥­ã®å‰Šæ¸›

ã©ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‹ã‚‰å®Ÿè£…ã‚’å§‹ã‚ã¾ã™ã‹ï¼Ÿ

ç‰¹å¾´ç”Ÿæˆã§ã®Kaggleãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ã‚’è©¦ã—ã¦ã¿ã‚‹

ã‹ã‹ã£ãŸæ™‚é–“ã‚’Json.dbã«è¨˜éŒ²ã—ã¦çµ‚äº†æ™‚åˆ»ã‚’è‰¯ãã†ã—ã‚„ã™ãã™ã‚‹ã®ã¯ï¼Ÿ
sqliteç¨‹ã˜ã‚ƒãªãã¦ã‚ˆã„æ°—ãŒã—ã¦ã‚‹

ç¢ºã‹ã«ã€ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã‚ˆãè¨­è¨ˆã•ã‚Œã¦ã„ã¦ã€å¤šãã®å„ªã‚ŒãŸæ©Ÿèƒ½ã‚’æŒã£ã¦ã„ã¾ã™ï¼

## ğŸ‘ ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å„ªã‚Œã¦ã„ã‚‹ç‚¹

### 1. **é«˜åº¦ãªæ¬ æå€¤å‡¦ç†**
```python
# 3ã¤ã®æˆ¦ç•¥ã‹ã‚‰é¸æŠå¯èƒ½
missing_strategy: ['basic', 'advanced', 'model_specific']
```
- ãƒ¢ãƒ‡ãƒ«ç‰¹æ€§ã«å¿œã˜ãŸæ¬ æå€¤å‡¦ç†
- æ¬ æãƒ•ãƒ©ã‚°ã®è‡ªå‹•ç”Ÿæˆï¼ˆ`_is_missing`ã‚«ãƒ©ãƒ ï¼‰
- Treeç³»ãƒ¢ãƒ‡ãƒ«ã¸ã®æœ€é©åŒ–

### 2. **åŒ…æ‹¬çš„ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**
```python
feature_engineer.add_numeric_features()
feature_engineer.add_categorical_features()
feature_engineer.add_interaction_features()
feature_engineer.add_datetime_features()
```
- 4ç¨®é¡ã®ç‰¹å¾´é‡ç”Ÿæˆã‚’ç¶²ç¾…
- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã§å†ç¾æ€§ã‚’ç¢ºä¿

### 3. **æŸ”è»Ÿãªæœ€é©åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³**
```python
if args.optimize:
    optimizer = create_quick_optimizer(base_model_name, args.problem_type, 'optuna', n_trials=args.n_trials)
```
- Optunaã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢
- ãƒˆãƒ©ã‚¤ã‚¢ãƒ«æ•°ã®èª¿æ•´å¯èƒ½

### 4. **Webhooké€šçŸ¥æ©Ÿèƒ½**
```python
@webhook_notify("Advanced Missing Workflow")
```
- å‡¦ç†ã®é€²æ—ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§é€šçŸ¥
- Discord/Slackå¯¾å¿œ

## ğŸš€ ã•ã‚‰ãªã‚‹æ”¹å–„ã®ææ¡ˆ

### 1. **ç‰¹å¾´é‡é¸æŠã®è¿½åŠ **
```python
# é‡è¦åº¦ã«ã‚ˆã‚‹ç‰¹å¾´é‡é¸æŠ
from sklearn.feature_selection import SelectFromModel
selector = SelectFromModel(best_model, threshold='median')
X_train_selected = selector.fit_transform(X_train_features, y_train)
```

### 2. **ã‚ˆã‚Šé«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«**
```python
# Blending/Stackingã®å®Ÿè£…
if args.ensemble:
    from mlxtend.classifier import StackingClassifier
    meta_model = LogisticRegression()
    stacking = StackingClassifier(
        classifiers=trained_models[:3],
        meta_classifier=meta_model,
        use_probas=True
    )
```

### 3. **äº¤å·®æ¤œè¨¼ã®å¼·åŒ–**
```python
# K-Fold CVã§ã‚ˆã‚Šå®‰å®šã—ãŸè©•ä¾¡
from sklearn.model_selection import StratifiedKFold
cv_scores = cross_val_score(model, X_train_features, y_train,
                           cv=StratifiedKFold(n_splits=5),
                           scoring='accuracy')
```

### 4. **å¾Œå‡¦ç†ã®æœ€é©åŒ–**
```python
# äºˆæ¸¬å€¤ã®å¾Œå‡¦ç†
def optimize_threshold(y_true, y_pred_proba):
    thresholds = np.arange(0.1, 0.9, 0.01)
    scores = [accuracy_score(y_true, y_pred_proba > t) for t in thresholds]
    return thresholds[np.argmax(scores)]
```

### 5. **ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®ãƒã‚§ãƒƒã‚¯**
```python
# ç‰¹å¾´é‡ã®ç›¸é–¢ãƒã‚§ãƒƒã‚¯
high_corr_features = correlation_matrix[correlation_matrix > 0.95]
if len(high_corr_features) > 0:
    print("Warning: Highly correlated features detected")
```

## ğŸ’¡ ã‚¹ã‚³ã‚¢æ”¹å–„ã®ãŸã‚ã®å…·ä½“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

1. **ç‰¹å¾´é‡ã®æ·±å €ã‚Š**
   - ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«åŸºã¥ãç‰¹å¾´é‡ä½œæˆ
   - å¤–ã‚Œå€¤ã®ç‰¹åˆ¥ãªå‡¦ç†

2. **ãƒ¢ãƒ‡ãƒ«ã®å¤šæ§˜æ€§**
   - Neural Network (TabNetç­‰)ã®è¿½åŠ 
   - ç•°ãªã‚‹seedã§ã®è¤‡æ•°ãƒ¢ãƒ‡ãƒ«

3. **æ¤œè¨¼æˆ¦ç•¥ã®æ”¹å–„**
   - Adversarial Validationã®å®Ÿæ–½
   - æ™‚ç³»åˆ—ã‚’è€ƒæ…®ã—ãŸåˆ†å‰²ï¼ˆã‚‚ã—é©ç”¨å¯èƒ½ãªã‚‰ï¼‰

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯æ—¢ã«é«˜å“è³ªã§ã™ãŒã€ä¸Šè¨˜ã®æ”¹å–„ç‚¹ã‚’åŠ ãˆã‚‹ã“ã¨ã§ã€ã•ã‚‰ãªã‚‹ã‚¹ã‚³ã‚¢å‘ä¸ŠãŒæœŸå¾…ã§ãã¾ã™ã€‚ç‰¹ã«ç‰¹å¾´é‡é¸æŠã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®å¼·åŒ–ãŒåŠ¹æœçš„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œæ™‚é–“ã‚’è¨˜éŒ²ã—ã¦ã€æ¬¡å›ã®çµ‚äº†è¦‹è¾¼ã¿æ™‚é–“ã‚’è¡¨ç¤ºã™ã‚‹ä»•çµ„ã¿ã¯éå¸¸ã«æœ‰åŠ¹ã ã¨æ€ã„ã¾ã™ï¼JSONãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ã†ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚‚ã€ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«ã¯é©ã—ã¦ã„ã¾ã™ã­ã€‚
JSONãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ã†ãƒ¡ãƒªãƒƒãƒˆ

ã‚·ãƒ³ãƒ—ãƒ«ã• - SQLiteã®ã‚ˆã†ãªå¤–éƒ¨ä¾å­˜ãŒãªã„
å¯èª­æ€§ - ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ‡ã‚£ã‚¿ã§ç›´æ¥ç¢ºèªãƒ»ç·¨é›†å¯èƒ½
ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç† - Gitã§å·®åˆ†ãŒè¦‹ã‚„ã™ã„
è»½é‡ - å°è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã«ã¯ååˆ†

å®Ÿè£…ä¾‹Workflow Execution Time Trackerã‚³ãƒ¼ãƒ‰Â import json
import time
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import statistics

class WorkflowTimeTracker:
    def __init__(self, db_path: str = "workflow_times.json"):
        self.db_path = db_pJSONæ§‹é€ ã®ä¾‹
ä¿å­˜ã•ã‚Œã‚‹JSONãƒ•ã‚¡ã‚¤ãƒ«ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ§‹é€ ã«ãªã‚Šã¾ã™ï¼š
json{
  "workflows": {
    "data_processing": {
      "executions": [
        {
          "timestamp": "2025-01-19T10:30:00",
          "duration": 45.2
        },
        {
          "timestamp": "2025-01-19T11:00:00",
          "duration": 42.8
        }
      ],
      "statistics": {
        "average": 44.0,
        "median": 44.0,
        "min": 42.8,
        "max": 45.2,
        "count": 2,
        "std_dev": 1.2
      }
    }
  }
}

import json
import time
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import statistics

class WorkflowTimeTracker:
    def __init__(self, db_path: str = "workflow_times.json"):
        self.db_path = db_path
        self.data = self._load_data()

    def _load_data(self) -> Dict:
        """Load existing data from JSON file"""
        if os.path.exists(self.db_path):
            try:
                with open(self.db_path, 'r') as f:
                    return json.load(f)
            except json.JSONDecodeError:
                return {"workflows": {}}
        return {"workflows": {}}

    def _save_data(self):
        """Save data to JSON file"""
        with open(self.db_path, 'w') as f:
            json.dump(self.data, f, indent=2, ensure_ascii=False)

    def start_workflow(self, workflow_name: str) -> float:
        """Start tracking a workflow execution"""
        start_time = time.time()

        # Get estimated completion time
        estimated_duration = self.get_estimated_duration(workflow_name)

        if estimated_duration:
            estimated_end = datetime.now() + timedelta(seconds=estimated_duration)
            print(f"ğŸš€ Starting workflow: {workflow_name}")
            print(f"â±ï¸  Estimated completion: {estimated_end.strftime('%H:%M:%S')} ({int(estimated_duration)}s)")
        else:
            print(f"ğŸš€ Starting workflow: {workflow_name} (first run, no estimate available)")

        return start_time

    def end_workflow(self, workflow_name: str, start_time: float):
        """End tracking and save the execution time"""
        duration = time.time() - start_time

        # Initialize workflow data if not exists
        if workflow_name not in self.data["workflows"]:
            self.data["workflows"][workflow_name] = {
                "executions": [],
                "statistics": {}
            }

        # Add new execution record
        execution = {
            "timestamp": datetime.now().isoformat(),
            "duration": duration
        }

        self.data["workflows"][workflow_name]["executions"].append(execution)

        # Keep only last 100 executions to prevent file bloat
        self.data["workflows"][workflow_name]["executions"] = \
            self.data["workflows"][workflow_name]["executions"][-100:]

        # Update statistics
        self._update_statistics(workflow_name)

        # Save to file
        self._save_data()

        print(f"âœ… Workflow completed in {duration:.2f}s")

    def _update_statistics(self, workflow_name: str):
        """Update statistics for a workflow"""
        executions = self.data["workflows"][workflow_name]["executions"]
        durations = [e["duration"] for e in executions]

        if durations:
            stats = {
                "average": statistics.mean(durations),
                "median": statistics.median(durations),
                "min": min(durations),
                "max": max(durations),
                "count": len(durations)
            }

            # Add standard deviation if we have enough samples
            if len(durations) >= 2:
                stats["std_dev"] = statistics.stdev(durations)

            self.data["workflows"][workflow_name]["statistics"] = stats

    def get_estimated_duration(self, workflow_name: str) -> Optional[float]:
        """Get estimated duration based on historical data"""
        if workflow_name not in self.data["workflows"]:
            return None

        stats = self.data["workflows"][workflow_name].get("statistics", {})

        if not stats:
            return None

        # Use weighted average of mean and median for more stable estimates
        if "median" in stats and "average" in stats:
            # Give more weight to median as it's less affected by outliers
            return (stats["median"] * 0.7 + stats["average"] * 0.3)

        return stats.get("average")

    def get_workflow_stats(self, workflow_name: str) -> Optional[Dict]:
        """Get statistics for a specific workflow"""
        if workflow_name in self.data["workflows"]:
            return self.data["workflows"][workflow_name]["statistics"]
        return None

    def list_workflows(self) -> List[str]:
        """List all tracked workflows"""
        return list(self.data["workflows"].keys())


# Usage example with context manager
class WorkflowTimer:
    def __init__(self, tracker: WorkflowTimeTracker, workflow_name: str):
        self.tracker = tracker
        self.workflow_name = workflow_name
        self.start_time = None

    def __enter__(self):
        self.start_time = self.tracker.start_workflow(self.workflow_name)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.start_time:
            self.tracker.end_workflow(self.workflow_name, self.start_time)


# Example usage
def main():
    tracker = WorkflowTimeTracker()

    # Method 1: Using context manager
    with WorkflowTimer(tracker, "data_processing"):
        # Your workflow code here
        time.sleep(2)  # Simulate work

    # Method 2: Manual tracking
    start = tracker.start_workflow("file_upload")
    # Your workflow code here
    time.sleep(1)  # Simulate work
    tracker.end_workflow("file_upload", start)

    # Get statistics
    stats = tracker.get_workflow_stats("data_processing")
    if stats:
        print(f"\nStatistics for 'data_processing':")
        print(f"  Average: {stats.get('average', 0):.2f}s")
        print(f"  Median: {stats.get('median', 0):.2f}s")
        print(f"  Min: {stats.get('min', 0):.2f}s")
        print(f"  Max: {stats.get('max', 0):.2f}s")
        print(f"  Count: {stats.get('count', 0)}")


if __name__ == "__main__":
    main()

Claude initã—ã¦ã„ãªã„ã€‚ã€‚ã€‚

### Makefile å†… â€œç²¾åº¦ãŒä¼¸ã³ã‚„ã™ã„â€ ã‚³ãƒãƒ³ãƒ‰é †ï¼ˆç›®å®‰ï¼‰

| é †ä½ | ã‚³ãƒãƒ³ãƒ‰                                     | æœŸå¾…ç²¾åº¦  | ã²ã¨è¨€ã§                                                             |
| -- | ---------------------------------------- | ----- | ---------------------------------------------------------------- |
| 1  | **`ensemble-optimized`**                 | â˜…â˜…â˜…â˜…â˜† | å„ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ â†’ Optuna/Bayesian ã§é‡ã¿æœ€é©åŒ–ã€‚æœ€ã‚‚ä¼¸ã³å¹…ãŒå¤§ãã„ãŒæ™‚é–“ã‚‚ã‹ã‹ã‚‹ã€‚               |
| 2  | **`ensemble-stacking`**                  | â˜…â˜…â˜…â˜…  | LightGBM/CatBoost/XGBoost ã‚’ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ã§å†å­¦ç¿’ã€‚å®Ÿè£…è² è·ã¯ä¸­ã€+0.0002ã€œ0.0004 è¦‹è¾¼ã‚ã‚‹ã€‚ |
| 3  | **`ensemble-average`**                   | â˜…â˜…â˜…â˜†  | ã„ã¡ã°ã‚“æ¥½ã€‚è¤‡æ•° seed ã® LGB + Cat ã‚’ rank å¹³å‡ã™ã‚‹ã ã‘ã§ +0.0003 å‰å¾Œã€‚             |
| 4  | **`optimize-optuna`**ï¼ˆâ†’ å„ `model-*` å†å­¦ç¿’ï¼‰ | â˜…â˜…â˜…   | å˜ä½“ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ©ã‚’ 30ã€œ50 trial æ¢ç´¢ã€‚LGB/CB/XGB ãŒ +0.0001ã€œ0.0003 æ”¹å–„ã€‚       |
| 5  | **`model-lgb`ï¼ˆDART æ¨å¥¨ï¼‰**                 | â˜…â˜…â˜†   | seed ã‚’ 5-10 å€‹å›ã™ã ã‘ã§ 0.9745â†’0.9750 ä»˜è¿‘ã«ä¹—ã‚Šã‚„ã™ã„ã€‚                       |
| 6  | **`model-cat`**                          | â˜…â˜…    | æ¬ æ/ã‚«ãƒ†ã‚´ãƒªè‡ªå‹•å‡¦ç†ã§æ‰‹é–“å°‘ã€‚LGB ã¨ä¸¦ã¹ã¦å¹³å‡ã™ã‚‹ã¨åŠ¹æœå¤§ã€‚                                |
| 7  | **`model-xgb`**                          | â˜…â˜†    | ä¸å‡è¡¡èª¿æ•´ãŒå¿…é ˆã€‚å˜ä½“ã§ã¯é ­æ‰“ã¡ã—ã‚„ã™ã„ãŒå¤šæ§˜æ€§è¦å“¡ã«ãªã‚‹ã€‚                                   |

> **æ—©ãç¢ºå®Ÿã«ã‚¹ã‚³ã‚¢ã‚’æŠ¼ã—ä¸Šã’ãŸã„ãªã‚‰**
> **â‘  `model-lgb` + `model-cat` ã‚’ seed å¤‰ãˆã¦å­¦ç¿’ â†’ â‘¡ `ensemble-average`**
> ã“ã‚Œã ã‘ã§ Public LB ãŒ 0.0003ã€œ0.0005 ä¸ŠãŒã‚‹ã‚±ãƒ¼ã‚¹ãŒå¤šã„ã§ã™ã€‚
> ç›®æ¨™ã‚¹ã‚³ã‚¢ã«å±Šã‹ãªã‘ã‚Œã°ã€**`ensemble-stacking` â†’ `ensemble-optimized`** ã¸æ®µéšçš„ã«æ‹¡å¼µã™ã‚‹ã®ãŒ ROI é«˜ã‚ã€‚


0H3KM:~/dev/my-study/ml/solid-ml-stack-s5e7$ make ensemble-optimized
python -c "from src.modeling.ensemble import create_optimized_ensemble; print('Optimized ensemble ready')"
/bin/sh: 1: python: not found
make: *** [Makefile:96: ensemble-optimized] Error 127

ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ä¾é ¼

è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯Duckdbã«æ ¼ç´æ¸ˆã¿
/home/wsl/dev/my-study/ml/kaggle-original-data-duckdb/kaggle_datasets.duckdb

- [ ] scikit-learnã§ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆRF, LGBM, XGBï¼‰
- [ ] validation.py ã«è©•ä¾¡æŒ‡æ¨™ãƒ­ã‚¸ãƒƒã‚¯è¿½åŠ 
- [ ] predict.pyã§äºˆæ¸¬ â†’ submission.csvç”Ÿæˆï¼ˆDuckDBã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼‰

ã“ã®Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã€ŒPlayground Series - Season 5, Episode 7ã€ã¯ã€**åˆå¿ƒè€…å‘ã‘ã®ã‚¿ãƒ–ãƒ©ãƒ¼ï¼ˆè¡¨å½¢å¼ï¼‰ãƒ‡ãƒ¼ã‚¿ç«¶æŠ€**ã§ã‚ã‚Šã€ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ†ãƒ¼ãƒãƒ»ç›®çš„ãƒ»è©•ä¾¡åŸºæº–ãƒ»ç«¶äº‰ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚Šã¾ã™ã€‚

---

## âœ… ãƒ†ãƒ¼ãƒãƒ»ç›®çš„

* **èª²é¡Œå†…å®¹**ï¼š
  ä¸ãˆã‚‰ã‚ŒãŸ **ç¤¾ä¼šçš„è¡Œå‹•ã‚„æ€§æ ¼ç‰¹æ€§ã«é–¢ã™ã‚‹ç‰¹å¾´é‡** ã‚’ã‚‚ã¨ã«ã€å„äººãŒã€Œ**Introvertï¼ˆå†…å‘çš„ï¼‰**ã€ã‹ã€Œ**Extrovertï¼ˆå¤–å‘çš„ï¼‰**ã€ã‹ã‚’ **2ã‚¯ãƒ©ã‚¹åˆ†é¡**ã™ã‚‹å•é¡Œã€‚

* **ç›®çš„**ï¼š
  åˆå¿ƒè€…ã§ã‚‚å–ã‚Šçµ„ã‚ã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚ŒãŸã€**è»½é‡ã‹ã¤å®Ÿè·µçš„ãªæ©Ÿæ¢°å­¦ç¿’ç·´ç¿’ç”¨ã‚³ãƒ³ãƒš**ã€‚ãƒ¢ãƒ‡ãƒ«ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã€ãƒ¢ãƒ‡ãƒ«é¸å®šã€å¯è¦–åŒ–ãªã©ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã‚‹ã€‚

---

## ğŸ§  ç«¶æŠ€ã§ç«¶ã†ãƒã‚¤ãƒ³ãƒˆ

* **äºˆæ¸¬ç²¾åº¦ï¼ˆAccuracyï¼‰ã‚’æœ€å¤§åŒ–**ã™ã‚‹ã“ã¨ãŒä¸»ãªç«¶æŠ€ç›®æ¨™ã§ã™ã€‚

* è©•ä¾¡æŒ‡æ¨™ã¯ä»¥ä¸‹ã®é€šã‚Šï¼š

  ```
  Accuracy = (æ­£ã—ãäºˆæ¸¬ã—ãŸæ•°) / (å…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿æ•°)
  ```

* **äºˆæ¸¬å¯¾è±¡**ï¼š
  ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®å„ `id` ã«å¯¾ã—ã¦ã€`Personality`ï¼ˆIntrovert / Extrovertï¼‰ã‚’äºˆæ¸¬ã—æå‡ºã€‚

* **æå‡ºå½¢å¼ä¾‹**ï¼š

  ```
  id,Personality
  18524,Extrovert
  18525,Introvert
  18526,Introvert
  ```

---

## ğŸ” é«˜ã‚¹ã‚³ã‚¢ã‚’ç‹™ã†ãŸã‚ã®å·¥å¤«ãƒ»è©•ä¾¡ãŒé«˜ã„æ¡ä»¶

| é …ç›®                 | è§£èª¬                                                                           |
| ------------------ | ---------------------------------------------------------------------------- |
| **ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**    | ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å¯è¦–åŒ–ã—ã€ç„¡é§„ãªç‰¹å¾´ã®å‰Šé™¤ã‚„æ„å‘³ã®ã‚ã‚‹å¤‰æ›ã‚’è¡Œã†ã“ã¨ãŒç²¾åº¦å‘ä¸Šã®éµã€‚                                   |
| **ãƒ¢ãƒ‡ãƒ«ã®é¸å®š**         | æ±ç”¨çš„ãªåˆ†é¡ãƒ¢ãƒ‡ãƒ«ï¼ˆLightGBM / CatBoost / XGBoost / RandomForest / Ensembleãªã©ï¼‰ã®æ¯”è¼ƒãƒ»æœ€é©åŒ–ã€‚ |
| **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´**    | Grid Search / Optuna ãªã©ã§éå­¦ç¿’ã‚’é¿ã‘ã¤ã¤ç²¾åº¦ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚                                   |
| **ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³**     | Overfitting å¯¾ç­–ã¨ã—ã¦ K-Fold ã‚„ Stratified K-Fold ã‚’æ´»ç”¨ã€‚                            |
| **ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†**         | æ¬ æå€¤å‡¦ç†ã€ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆOneHot/Labelãªã©ï¼‰ãŒé‡è¦ã€‚                                    |
| **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«**         | è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§å®‰å®šæ€§ã¨ç²¾åº¦ã‚’å‘ä¸Šå¯èƒ½ã€‚                                               |
| **EDA (æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ)** | ç‰¹å¾´é‡é–“ã®ç›¸é–¢ã‚„åˆ†å¸ƒã‚’æŠŠæ¡ã—ã€è¨­è¨ˆã®ãƒ’ãƒ³ãƒˆã‚’å¾—ã‚‹ã€‚                                                    |

---

## ğŸ† è©•ä¾¡æŒ‡æ¨™ã¨å ±é…¬

* **è©•ä¾¡æŒ‡æ¨™**ï¼šAccuracyï¼ˆç²¾åº¦ï¼‰
* **è³å“**ï¼šä¸Šä½è€…ã«ã¯ **Kaggleé™å®šã‚°ãƒƒã‚º**ï¼ˆTã‚·ãƒ£ãƒ„ã‚„ã‚¹ãƒ†ãƒƒã‚«ãƒ¼ç­‰ï¼‰â€»ãŸã ã—ã‚·ãƒªãƒ¼ã‚ºå†…ã§1å›é™ã‚Š
* **ãƒ¡ãƒ€ãƒ«ã‚„ãƒã‚¤ãƒ³ãƒˆã®ä»˜ä¸ã¯ãªã—**ï¼ˆã‚ãã¾ã§ç·´ç¿’ç”¨ï¼‰

---

## ğŸ’¡ ã¾ã¨ã‚ï¼šã“ã®ã‚³ãƒ³ãƒšã§å¾—ã‚‰ã‚Œã‚‹ã“ã¨

* åˆå¿ƒè€…ã€œä¸­ç´šè€…ã«ã¨ã£ã¦ã€**åˆ†é¡å•é¡Œã®ç·åˆæ¼”ç¿’**ã¨ã—ã¦éå¸¸ã«å„ªã‚Œã¦ã„ã‚‹ã€‚
* ç‰¹ã«ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ«å‘ä¸Šã«è²¢çŒ®ï¼š

  * ç‰¹å¾´é‡è¨­è¨ˆ
  * ãƒ¢ãƒ‡ãƒ«é¸å®šã¨è©•ä¾¡
  * ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
  * ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ï¼ˆscikit-learn ã‚„ PyCaret ãªã©ï¼‰

---

èˆˆå‘³ãŒã‚ã‚Œã°ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆä¾‹ï¼šLightGBM + StratifiedKFoldï¼‰ã‚„ã€ä¸Šä½è€…ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®è§£èª¬ã‚‚æä¾›å¯èƒ½ã§ã™ã€‚ã”å¸Œæœ›ãŒã‚ã‚Œã°ãŠç”³ã—ä»˜ã‘ãã ã•ã„ã€‚

kaggle-original-data-duckdbã®å®Ÿè£…ã¨å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ä¾é ¼

# kaggle-original-data-duckdb

ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ã€è¤‡æ•°ã®Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®CSVãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å˜ä¸€ã®DuckDBãƒ•ã‚¡ã‚¤ãƒ«ã§ç®¡ç†ã—ã€ã‚¹ã‚­ãƒ¼ãƒåˆ¥ã«æ•´ç†ã—ã¾ã™ã€‚
ä¸€ã¤ã®DuckDBã§è¤‡æ•°ã®ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒ¼ãƒã§åŒºåˆ‡ã£ã¦ç®¡ç†ã—ã€åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿æ¢ç´¢ã¨åˆ†æã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚

## ğŸš€ æ©Ÿèƒ½

- **è‡ªå‹•ã‚¹ã‚­ãƒ¼ãƒä½œæˆ**: ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³åã®ãƒã‚¤ãƒ•ãƒ³ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«ç½®æ›ã—ã¦ã‚¹ã‚­ãƒ¼ãƒåã‚’æ­£è¦åŒ–
- **CSVãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•èª­ã¿è¾¼ã¿**: å„ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ•ã‚©ãƒ«ãƒ€å†…ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•çš„ã«ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦èª­ã¿è¾¼ã¿
- **ã‚¹ã‚­ãƒ¼ãƒãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§è¡¨ç¤º**: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®ã‚¹ã‚­ãƒ¼ãƒã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä¸€è¦§ã‚’è¡¨ç¤º
- **ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªå®Ÿè¡Œ**: ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ã®ãŸã‚ã®ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ
- **ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½**: ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
- **Makefileçµ±åˆ**: ç°¡å˜ãªã‚³ãƒãƒ³ãƒ‰ã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†

## ğŸ“‹ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# Makefileã‚’ä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰
make install

# ã¾ãŸã¯ç›´æ¥pipã‚’ä½¿ç”¨
pip install -r requirements.txt
```

### 2. åˆæœŸã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
make setup
```

ã“ã®ã‚³ãƒãƒ³ãƒ‰ã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
- ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
- Kaggleãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª

## ğŸ—‚ï¸ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ï¼š

```
ml/
â”œâ”€â”€ kaggle-original-data/                    # ã€å»ƒæ­¢ã€‘CSVãƒ•ã‚¡ã‚¤ãƒ«ã¯ä½¿ç”¨ã—ãªã„
â”‚   â”œâ”€â”€ playground-series-s5e7/
â”‚   â”‚   â”œâ”€â”€ train.csv â†’ DuckDBã«ç§»è¡Œæ¸ˆã¿
â”‚   â”‚   â”œâ”€â”€ test.csv â†’ DuckDBã«ç§»è¡Œæ¸ˆã¿
â”‚   â”‚   â””â”€â”€ sample_submission.csv â†’ DuckDBã«ç§»è¡Œæ¸ˆã¿
â”‚   â””â”€â”€ cmi-detect-behavior-with-sensor-data/
â”‚       â”œâ”€â”€ train.csv â†’ DuckDBã«ç§»è¡Œæ¸ˆã¿
â”‚       â”œâ”€â”€ test.csv â†’ DuckDBã«ç§»è¡Œæ¸ˆã¿
â”‚       â”œâ”€â”€ train_demographics.csv â†’ DuckDBã«ç§»è¡Œæ¸ˆã¿
â”‚       â””â”€â”€ test_demographics.csv â†’ DuckDBã«ç§»è¡Œæ¸ˆã¿
â”œâ”€â”€ kaggle-original-data-duckdb/             # ã€ãƒ¡ã‚¤ãƒ³ã€‘ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
â”‚   â”œâ”€â”€ kaggle_datasets.duckdb              # å…¨ãƒ‡ãƒ¼ã‚¿ãŒã“ã“ã«æ ¼ç´æ¸ˆã¿
â”‚   â”œâ”€â”€ kaggle_duckdb_manager.py            # ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   â”œâ”€â”€ requirements.txt                    # Pythonä¾å­˜é–¢ä¿‚
â”‚   â”œâ”€â”€ Makefile                           # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†
â”‚   â”œâ”€â”€ README.md                          # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â””â”€â”€ kaggle_duckdb.log                  # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
```

### ã‚¹ã‚­ãƒ¼ãƒå‘½åè¦å‰‡

ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³åã®ãƒã‚¤ãƒ•ãƒ³ï¼ˆ`-`ï¼‰ã¯ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ï¼ˆ`_`ï¼‰ã«å¤‰æ›ã•ã‚Œã¾ã™ï¼š

- `playground-series-s5e7` â†’ `playground_series_s5e7`
- `cmi-detect-behavior-with-sensor-data` â†’ `cmi_detect_behavior_with_sensor_data`

## ğŸ› ï¸ ä½¿ç”¨æ–¹æ³•

### Makefileã‚³ãƒãƒ³ãƒ‰

```bash
# ãƒ˜ãƒ«ãƒ—è¡¨ç¤º
make help

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çŠ¶æ…‹ç¢ºèª
make status

# ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
make import-data

# ã‚¹ã‚­ãƒ¼ãƒã¨ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§è¡¨ç¤º
make list-schemas

# ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
make sample-queries

# ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
make export-data

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è¡¨ç¤º
make logs

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†æ§‹ç¯‰
make rebuild

# ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
make clean
```

### ç›´æ¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ

```bash
# ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
python kaggle_duckdb_manager.py --import-data

# ã‚¹ã‚­ãƒ¼ãƒä¸€è¦§è¡¨ç¤º
python kaggle_duckdb_manager.py --list-schemas

# ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
python kaggle_duckdb_manager.py --sample-queries

# å¯¾è©±ãƒ¢ãƒ¼ãƒ‰
python kaggle_duckdb_manager.py --interactive
```

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œ

### DuckDBã‚¯ã‚¨ãƒªä¾‹

```sql
-- ã‚¹ã‚­ãƒ¼ãƒä¸€è¦§è¡¨ç¤º
SELECT schema_name FROM information_schema.schemata;

-- ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§è¡¨ç¤º
SELECT table_name FROM information_schema.tables WHERE table_schema = 'playground_series_s5e7';

-- ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
SELECT * FROM playground_series_s5e7.train LIMIT 10;

-- ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ç¢ºèª
SELECT COUNT(*) FROM cmi_detect_behavior_with_sensor_data.train;

-- ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±è¡¨ç¤º
DESCRIBE playground_series_s5e7.train;
```

## ğŸ“ˆ ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

### playground_series_s5e7 ã‚¹ã‚­ãƒ¼ãƒ
- `train`: 18,524è¡Œ
- `test`: 6,175è¡Œ
- `sample_submission`: 6,175è¡Œ

### cmi_detect_behavior_with_sensor_data ã‚¹ã‚­ãƒ¼ãƒ
- `train`: 574,945è¡Œ
- `test`: 107è¡Œ
- `train_demographics`: 81è¡Œ
- `test_demographics`: 2è¡Œ

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

1. **Kaggleãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚‰ãªã„**
   ```bash
   # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
   make status

   # æ­£ã—ã„ãƒ‘ã‚¹ã‚’è¨­å®š
   # Makefileå†…ã® KAGGLE_DATA_DIR å¤‰æ•°ã‚’ç¢ºèª
   ```

2. **DuckDBãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ãŸå ´åˆ**
   ```bash
   # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’å†æ§‹ç¯‰
   make rebuild
   ```

3. **ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãããªã£ãŸå ´åˆ**
   ```bash
   # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªã‚¢
   make clear-logs
   ```

4. **ä¾å­˜é–¢ä¿‚ã®å•é¡Œ**
   ```bash
   # ä¾å­˜é–¢ä¿‚ã‚’å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
   make install
   ```

### ãƒ­ã‚°ã®ç¢ºèª

```bash
# æœ€æ–°ã®ãƒ­ã‚°ã‚’è¡¨ç¤º
make logs

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥ç¢ºèª
tail -f kaggle_duckdb.log
```

## ğŸ“ é–‹ç™ºè€…å‘ã‘æƒ…å ±

### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

- `kaggle_duckdb_manager.py`: ãƒ¡ã‚¤ãƒ³ã®ç®¡ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- `Makefile`: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã¨ã‚¿ã‚¹ã‚¯è‡ªå‹•åŒ–
- `requirements.txt`: Pythonä¾å­˜é–¢ä¿‚
- `kaggle_datasets.duckdb`: DuckDBãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«

### æ–°ã—ã„ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®è¿½åŠ 

1. Kaggleãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æ–°ã—ã„ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ•ã‚©ãƒ«ãƒ€ã‚’é…ç½®
2. `make import-data` ã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
3. `make list-schemas` ã§ã‚¹ã‚­ãƒ¼ãƒãŒæ­£ã—ãä½œæˆã•ã‚ŒãŸã“ã¨ã‚’ç¢ºèª

### ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

```bash
# åŸºæœ¬çš„ãªãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
make test

# å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ã§ãƒ†ã‚¹ãƒˆ
make interactive
```

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯å†…éƒ¨é–‹ç™ºç”¨ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚

## ğŸ¤ è²¢çŒ®

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ”¹å–„ææ¡ˆã‚„ãƒã‚°å ±å‘Šã¯ã€ãƒãƒ¼ãƒ å†…ã§å…±æœ‰ã—ã¦ãã ã•ã„ã€‚

---

**æœ€çµ‚æ›´æ–°**: 2024å¹´12æœˆ
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0.0

# Solid ML Stack

é«˜é€Ÿã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªKaggleç‰¹åŒ–æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

## ç‰¹å¾´

### ğŸ¯ Kaggleæœ€é©åŒ–
- **ã‚³ãƒ³ãƒšæå‡ºã¾ã§ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–**: å‰å‡¦ç†â†’ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°â†’ãƒ¢ãƒ‡ãƒ«å­¦ç¿’â†’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«â†’æå‡ºã®ä¸€é€£ã®ãƒ•ãƒ­ãƒ¼ã‚’è‡ªå‹•åŒ–
- **å†åˆ©ç”¨å¯èƒ½ãªè¨­è¨ˆ**: é–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹ãƒ™ãƒ¼ã‚¹ã§æ§‹æˆã—ã€ç•°ãªã‚‹ã‚³ãƒ³ãƒšã§ã‚‚ç°¡å˜ã«å†åˆ©ç”¨å¯èƒ½
- **CPUç‰¹åŒ–**: XGBoost/LightGBM/CatBoostãªã©ãƒ„ãƒªãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä¸­å¿ƒã¨ã—ãŸé«˜é€Ÿå­¦ç¿’

### ğŸ”§ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–è¨­è¨ˆ
- **å‰å‡¦ç†**: æ¬ æå€¤å‡¦ç†ã€å¤–ã‚Œå€¤é™¤å»ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- **ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**: æ•°å€¤å¤‰æ›ã€ã‚«ãƒ†ã‚´ãƒªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€äº¤äº’ä½œç”¨ç‰¹å¾´é‡ã€æ™‚ç³»åˆ—ç‰¹å¾´é‡
- **ãƒ¢ãƒ‡ãƒ«å­¦ç¿’**: XGBoostã€LightGBMã€CatBoostã€ç·šå½¢ãƒ¢ãƒ‡ãƒ«
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢**: Grid Searchã€Random Searchã€Bayesian Optimizationã€Optuna
- **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«**: å¹³å‡åŒ–ã€é‡ã¿ä»˜ãå¹³å‡ã€ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã€Voting

### ğŸ“Š ã‚¿ãƒ–ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ç‰¹åŒ–
- CSVãªã©ã®è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã«ç‰¹åŒ–
- pandasâ†’scikit-learnãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹
- ç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ»æ™‚ç³»åˆ—Deep Learningã¯å¯¾è±¡å¤–

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# åŸºæœ¬ä¾å­˜é–¢ä¿‚
pip install -e .

# æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
pip install -e .[optimization]

# å¯è¦–åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
pip install -e .[visualization]

# é–‹ç™ºç”¨ãƒ„ãƒ¼ãƒ«
pip install -e .[dev]
```

## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### åŸºæœ¬çš„ãªä½¿ã„æ–¹

```python
import pandas as pd
from preprocessing import Preprocessor
from features.engineering import AutoFeatureEngineer
from modeling import ModelFactory
from submission import SubmissionGenerator

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆDuckDBã‹ã‚‰ï¼‰
from src.data.data_loader import DataLoader
from src.config.kaggle_config import KaggleConfig

config = KaggleConfig()
data_loader = DataLoader(config)
train_df, test_df = data_loader.load_train_test()

# å‰å‡¦ç†
preprocessor = Preprocessor()
X_train, X_val, y_train, y_val = preprocessor.prepare_data(train_df, 'target')
X_train_processed, X_val_processed = preprocessor.process_train_test(X_train, X_val, y_train)

# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
feature_engineer = AutoFeatureEngineer()
X_train_features = feature_engineer.fit_transform(X_train_processed, y_train)
X_val_features = feature_engineer.pipeline.transform(X_val_processed)

# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
factory = ModelFactory()
models = factory.get_default_models(target_type='regression')

trained_models = {}
for model in models:
    model.fit(X_train_features, y_train, X_val_features, y_val)
    trained_models[model.config.name] = model

# äºˆæ¸¬ãƒ»æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
X_test_processed = preprocessor.transform(test_df.drop(columns=['id']))
X_test_features = feature_engineer.pipeline.transform(X_test_processed)

predictions = {name: model.predict(X_test_features)
               for name, model in trained_models.items()}

submission_gen = SubmissionGenerator()
submission_path = submission_gen.create_ensemble_submission(
    predictions, test_df['id'].values, 'target', 'id'
)
```

### ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ

```bash
# ãƒ•ãƒ«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œï¼ˆDuckDBã‹ã‚‰è‡ªå‹•å–å¾—ï¼‰
python3 scripts/kaggle_workflow.py \
    --target-col Personality \
    --problem-type classification \
    --optimize \
    --ensemble

# Makefileã‚’ä½¿ã£ãŸå®Ÿè¡Œ
make kaggle-classification TARGET=Personality
make personality-prediction
```

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
src/
â”œâ”€â”€ preprocessing/          # å‰å‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
â”‚   â”œâ”€â”€ preprocessor.py    # ãƒ¡ã‚¤ãƒ³å‰å‡¦ç†ã‚¯ãƒ©ã‚¹
â”‚   â”œâ”€â”€ pipeline.py        # å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
â”‚   â””â”€â”€ transformers.py    # å€‹åˆ¥å¤‰æ›å™¨
â”œâ”€â”€ features/              # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
â”‚   â””â”€â”€ engineering/       # ç‰¹å¾´é‡ç”Ÿæˆ
â”‚       â”œâ”€â”€ base.py        # ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ç”Ÿæˆå™¨
â”‚       â”œâ”€â”€ numeric.py     # æ•°å€¤ç‰¹å¾´é‡
â”‚       â”œâ”€â”€ categorical.py # ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡
â”‚       â”œâ”€â”€ interaction.py # äº¤äº’ä½œç”¨ç‰¹å¾´é‡
â”‚       â”œâ”€â”€ datetime.py    # æ—¥æ™‚ç‰¹å¾´é‡
â”‚       â”œâ”€â”€ aggregation.py # é›†ç´„ç‰¹å¾´é‡
â”‚       â””â”€â”€ pipeline.py    # ç‰¹å¾´é‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
â”œâ”€â”€ modeling/              # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
â”‚   â”œâ”€â”€ base.py           # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
â”‚   â”œâ”€â”€ tree_models.py    # ãƒ„ãƒªãƒ¼ãƒ¢ãƒ‡ãƒ«ï¼ˆXGBoostã€LightGBMã€CatBoostï¼‰
â”‚   â”œâ”€â”€ linear_models.py  # ç·šå½¢ãƒ¢ãƒ‡ãƒ«
â”‚   â”œâ”€â”€ ensemble.py       # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•
â”‚   â””â”€â”€ factory.py        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒª
â”œâ”€â”€ optimization/          # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢
â”‚   â”œâ”€â”€ base.py           # ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ã‚¯ãƒ©ã‚¹
â”‚   â”œâ”€â”€ grid_search.py    # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ
â”‚   â”œâ”€â”€ random_search.py  # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ
â”‚   â”œâ”€â”€ bayesian_optimization.py # ãƒ™ã‚¤ã‚¸ã‚¢ãƒ³æœ€é©åŒ–
â”‚   â”œâ”€â”€ optuna_optimizer.py # Optunaæœ€é©åŒ–
â”‚   â””â”€â”€ factory.py        # æœ€é©åŒ–ãƒ•ã‚¡ã‚¯ãƒˆãƒª
â”œâ”€â”€ evaluation/           # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
â”‚   â””â”€â”€ metrics.py        # è©•ä¾¡æŒ‡æ¨™
â”œâ”€â”€ submission/           # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
â”‚   â””â”€â”€ submission_generator.py
â””â”€â”€ utils/                # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    â”œâ”€â”€ base.py           # ãƒ™ãƒ¼ã‚¹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    â”œâ”€â”€ config.py         # è¨­å®šç®¡ç†
    â””â”€â”€ io.py             # ãƒ•ã‚¡ã‚¤ãƒ«å…¥å‡ºåŠ›
```

## ä½¿ç”¨ä¾‹

### 1. ã‚«ã‚¹ã‚¿ãƒ å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```python
from preprocessing import PreprocessingPipeline
from preprocessing.transformers import *

# ã‚«ã‚¹ã‚¿ãƒ å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
pipeline = PreprocessingPipeline()
pipeline.add_step('missing', MissingValueHandler(numeric_strategy='median'))
pipeline.add_step('outliers', OutlierHandler(method='zscore', threshold=3))
pipeline.add_step('encoding', CategoricalEncoder(method='target'))
pipeline.add_step('scaling', NumericScaler(method='robust'))

X_processed = pipeline.fit_transform(X_train, y_train)
```

### 2. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

```python
from features.engineering import FeatureEngineeringPipeline

# ç‰¹å¾´é‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
fe_pipeline = FeatureEngineeringPipeline()
fe_pipeline.add_numeric_features(['log', 'sqrt', 'square'])
fe_pipeline.add_categorical_features(min_frequency=5)
fe_pipeline.add_interaction_features(max_interactions=100)
fe_pipeline.add_polynomial_features(degree=2)

X_features = fe_pipeline.fit_transform(X_train, y_train)
```

### 3. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–

```python
from optimization import OptunaOptimizer, OptimizationConfig

# Optunaæœ€é©åŒ–
search_space = {
    'n_estimators': {'type': 'int', 'low': 100, 'high': 1000},
    'max_depth': {'type': 'int', 'low': 3, 'high': 10},
    'learning_rate': {'type': 'loguniform', 'low': 0.01, 'high': 0.3}
}

config = OptimizationConfig(search_space=search_space, n_trials=100)
optimizer = OptunaOptimizer(config)

result = optimizer.optimize(model, X_train, y_train, X_val, y_val)
best_model = result.apply_best_params()
```

### 4. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«

```python
from modeling import StackingEnsemble, create_kaggle_models

# å¤šæ§˜ãªãƒ¢ãƒ‡ãƒ«ä½œæˆ
models = create_kaggle_models(target_type='regression')

# ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
stacking_model = StackingEnsemble(base_models=models, cv_folds=5)
stacking_model.fit(X_train, y_train, X_val, y_val)

predictions = stacking_model.predict(X_test)
```

## ã‚³ãƒ³ãƒ•ã‚£ã‚°è¨­å®š

```python
from config.kaggle_config import KaggleConfig, ConfigPresets

# å›å¸°å•é¡Œç”¨è¨­å®š
config = ConfigPresets.regression_competition()

# åˆ†é¡å•é¡Œç”¨è¨­å®š
config = ConfigPresets.classification_competition()

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
config = KaggleConfig(
    problem_type='regression',
    preprocessing={
        'handle_missing': True,
        'handle_outliers': True,
        'outlier_threshold': 2.0
    },
    feature_engineering={
        'numeric_features': True,
        'polynomial_features': True,
        'max_interactions': 150
    }
)
```

## ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

```bash
# å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
make test

# é«˜é€Ÿãƒ†ã‚¹ãƒˆï¼ˆslowãƒãƒ¼ã‚«ãƒ¼ã‚’é™¤å¤–ï¼‰
make test-fast

# ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã®ã¿
make test-unit

# çµ±åˆãƒ†ã‚¹ãƒˆã®ã¿
make test-integration

# ã‚«ãƒãƒ¬ãƒƒã‚¸ä»˜ããƒ†ã‚¹ãƒˆ
make test-coverage

# ã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ
make test-smoke
```

## é–‹ç™ºã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

### ã‚³ãƒ¼ãƒ‰å“è³ª
- **å‹ãƒ’ãƒ³ãƒˆ**: å…¨ã¦ã®é–¢æ•°ãƒ»ãƒ¡ã‚½ãƒƒãƒ‰ã«å‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
- **docstring**: ä¸»è¦ã‚¯ãƒ©ã‚¹ãƒ»é–¢æ•°ã«Googleã‚¹ã‚¿ã‚¤ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- **ãƒ†ã‚¹ãƒˆ**: pytestã«ã‚ˆã‚‹å˜ä½“ãƒ†ã‚¹ãƒˆ
- **ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: blackã«ã‚ˆã‚‹è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- **CPUæœ€é©åŒ–**: GPUä¸è¦ã§ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§é«˜é€Ÿå®Ÿè¡Œ
- **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã§ã‚‚ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ã«å‡¦ç†
- **ä¸¦åˆ—å‡¦ç†**: å¯èƒ½ãªç®‡æ‰€ã§ã®ä¸¦åˆ—åŒ–å®Ÿè£…

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
- **ç§˜åŒ¿æƒ…å ±**: API ã‚­ãƒ¼ã‚„èªè¨¼æƒ…å ±ã®ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ç¦æ­¢
- **å…¥åŠ›æ¤œè¨¼**: å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®é©åˆ‡ãªæ¤œè¨¼ãƒ»ã‚µãƒ‹ã‚¿ã‚¤ã‚º

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License

## è²¢çŒ®

1. ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ•ã‚©ãƒ¼ã‚¯
2. æ©Ÿèƒ½ãƒ–ãƒ©ãƒ³ãƒã‚’ä½œæˆ (`git checkout -b feature/amazing-feature`)
3. å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆ (`git commit -m 'Add amazing feature'`)
4. ãƒ–ãƒ©ãƒ³ãƒã«ãƒ—ãƒƒã‚·ãƒ¥ (`git push origin feature/amazing-feature`)
5. Pull Requestã‚’ä½œæˆ

## ã‚µãƒãƒ¼ãƒˆ

- Issues: GitHub Issues ã§è³ªå•ãƒ»ãƒã‚°å ±å‘Š
- Discussions: GitHub Discussions ã§ä¸€èˆ¬çš„ãªè­°è«–


ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼

ä¸¦åˆ—åŒ–ã®å®Ÿè£…

  - ç¾çŠ¶: 6ãƒ¢ãƒ‡ãƒ«ãŒé †æ¬¡è¨“ç·´ï¼ˆç´„6å€ã®æ™‚é–“ï¼‰
  - æœ€é©åŒ–: joblib.Parallelã§ä¸¦åˆ—å®Ÿè¡Œ
  - äºˆæƒ³åŠ¹æœ: 4-6å€é«˜é€ŸåŒ–

  2. ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´

  - LightGBM: n_jobs=-1ã§ä¸¦åˆ—åŒ–
  - XGBoost: n_jobs=-1, tree_method='hist'ã§é«˜é€ŸåŒ–
  - RandomForest: n_jobs=-1ã¯æ—¢ã«è¨­å®šæ¸ˆã¿

  3. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚­ãƒ£ãƒƒã‚·ãƒ¥

  - å‰å‡¦ç†çµæœã‚’pickleã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥
  - 2å›ç›®ä»¥é™ã®å®Ÿè¡Œã§å¤§å¹…çŸ­ç¸®

  4. CVæˆ¦ç•¥ã®æœ€é©åŒ–

  - ä¸è¦ãªé‡è¤‡CVã‚’å‰Šé™¤
  - å¿…è¦æœ€å°é™ã®CVã®ã¿å®Ÿè¡Œ

  ğŸ–¥ï¸ ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æœ€é©åŒ–æ¡ˆ

  ç¾åœ¨ã®ç’°å¢ƒã«å¯¾ã™ã‚‹å³åŠ¹æ€§ã®é«˜ã„æ”¹å–„:

  1. CPUä¸¦åˆ—åŒ–: ç¾åœ¨ã®n_jobs=-1ã‚’æœ€å¤§é™æ´»ç”¨
  2. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›: ä¸è¦ãªãƒ‡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼ã‚’å‰Šé™¤
  3. ãƒ‡ã‚£ã‚¹ã‚¯I/Oæœ€é©åŒ–: SSDä½¿ç”¨ã€ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šæ¸›

  å°†æ¥ã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æŠ•è³‡ï¼ˆOMEN 35Lç­‰ï¼‰ã§ã®åŠ¹æœ:
  - CPU: 8ã‚³ã‚¢ä»¥ä¸Š â†’ ä¸¦åˆ—è¨“ç·´ã§3-4å€é«˜é€ŸåŒ–
  - ãƒ¡ãƒ¢ãƒª: 32GBä»¥ä¸Š â†’ å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å®‰å®šæ€§å‘ä¸Š
  - SSD: NVMe â†’ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿2-3å€é«˜é€ŸåŒ–

  ğŸ’¡ å®Ÿè£…å„ªå…ˆåº¦

  | é …ç›®       | å®Ÿè£…é›£æ˜“åº¦ | åŠ¹æœ  | æ¨å¥¨åº¦ |
  |----------|-------|-----|-----|
  | ä¸¦åˆ—åŒ–å®Ÿè£…    | ä¸­     | é«˜   | â­â­â­ |
  | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´  | ä½     | ä¸­   | â­â­â­ |
  | ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ | ä½     | ä¸­   | â­â­  |
  | CVæœ€é©åŒ–    | ä¸­     | ä¸­   | â­â­  |

  ã“ã‚Œã‚‰ã®æœ€é©åŒ–ã«ã‚ˆã‚Šã€ç¾åœ¨ã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§ã‚‚2-4å€ã®é«˜é€ŸåŒ–ãŒæœŸå¾…ã§ãã¾ã™ã€‚

    1. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã®è©³ç´°æ¤œè¨¼ï¼š
  python3 scripts/detailed_leakage_check.py
  2. éå­¦ç¿’åˆ†æï¼š
  python3 scripts/overfitting_analysis.py
  3. é«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼š
  python3 scripts/enhanced_ensemble_workflow.py
  4. åŒ…æ‹¬çš„åˆ†æï¼š
  python3 src/analysis/comprehensive_analysis.py

   ä»¥ä¸‹ã®é«˜åº¦ãªåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼š

  ğŸ¯ å®Ÿè¡Œæ¸ˆã¿åˆ†æ

  1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼: CV 0.9690
  2. æœ€é©åŒ–è¨­å®š: CV 0.9680
  3. ç‰¹å¾´é‡åˆ†æ: 44ç‰¹å¾´é‡ã®é‡è¦åº¦ã¨SHAPè§£æå®Œäº†
  4. ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯æ¤œè¨¼: å¼·ã„ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’ç¢ºèª (åˆæˆãƒ‡ãƒ¼ã‚¿)

  ğŸ“Š ä¸»ãªç™ºè¦‹

  - ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯: Stage_fear ã¨ Drained_after_socializing ãŒ97%ã®ç²¾åº¦ã§äºˆæ¸¬å¯èƒ½
  - é‡è¦ç‰¹å¾´é‡: sqft, year_built, location ç³»ã®äº¤äº’ä½œç”¨ç‰¹å¾´é‡
  - ãƒ¢ãƒ‡ãƒ«æ€§èƒ½: XGBoost ãŒæœ€é«˜ (CV: 0.9680)

  ğŸ” åˆ©ç”¨å¯èƒ½ãªä»–ã®åˆ†æ

  # åŒ…æ‹¬çš„åˆ†æ (ä¿®æ­£ãŒå¿…è¦)
  python3 src/analysis/comprehensive_analysis.py

  # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ (imblearnè¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)
  python3 scripts/data_augmentation.py

  ç¾åœ¨ã®ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢: 0.9680 (æœ€é©åŒ–è¨­å®š)


wsl@DESKTOP-M40H3KM:~/dev/my-study/ml/solid-ml-stack-s5e7$


1058ä½ã§2240ãƒãƒ¼ãƒ ä¸­ã ã¨ä¸Šä½47%ãã‚‰ã„ã§ã™ã­ï¼éŠ…ãƒ¡ãƒ€ãƒ«ã¯é€šå¸¸ä¸Šä½10%ãã‚‰ã„ãªã®ã§ã€ã¾ã æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã­ã€‚

  97.49%ã¯è‰¯ã„ã‚¹ã‚³ã‚¢ã§ã™ãŒã€ä¸Šä½é™£ã¯ã‚‚ã£ã¨é«˜ã„ç²¾åº¦ã‚’å‡ºã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã•ã‚‰ãªã‚‹æ”¹å–„ã®ã‚¢ã‚¤ãƒ‡ã‚¢ï¼š

  1. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
  - æ–°ã—ã„ç‰¹å¾´é‡ã®ä½œæˆ
  - ç‰¹å¾´é‡ã®ç›¸äº’ä½œç”¨é …
  - å¤šé …å¼ç‰¹å¾´é‡

  2. ãƒ¢ãƒ‡ãƒ«ã®æ”¹è‰¯
  - ã‚ˆã‚Šå¤šæ§˜ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è¿½åŠ 
  - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–
  - ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«

  3. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
  - ç•°å¸¸å€¤å‡¦ç†
  - ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®æœ€é©åŒ–
  - ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ã®å¯¾å‡¦

  4. é«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
  - ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°
  - è¤‡æ•°ãƒ¬ãƒ™ãƒ«ã®ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°
