# Kaggle機械学習プロジェクトの再構築計画

## 背景と目標

現在のKaggleコンペプロジェクト（リポジトリ「solid-ml-stack-s5e7」）は、コードが煩雑で構造も複雑になっており、開発効率やモデル性能に悪影響を及ぼしています。実際、小規模なKaggleコンペにもかかわらず**Pythonファイル76個・コード約15,000行**に及ぶ過度な抽象化が見られ、チーム開発や保守が困難な状況です。

このままでは迅速な実験や安定したモデル構築が難しく、**Kaggle銅メダル（上位入賞）**という目標達成の障害となっています。そこで、本プロジェクトを一から整理し直し、効果的なノウハウを取り入れたシンプルで強固な構成へ再設計します。その目的は、開発効率とモデルの汎化性能を高め、Kaggle本番の**プライベートリーダーボードでも安定して上位に入賞すること**です。

## 現行プロジェクトの問題点

現状のコードベースを分析したところ、以下のような問題点が判明しました。

### アーキテクチャの過剰な複雑さ

* **過度な抽象化と層の重複**: コンペ規模に対して設計が複雑すぎます。例えば特徴量前処理に関して`Preprocessor`クラスと`FeatureEngineeringPipeline`クラスが存在し、役割が重複しています。モデル構築も`ModelFactory`パターンを採用していますが、このファクトリ実装が冗長で理解・保守が困難です。結果として、コード全体が読みづらく修正しにくい状態です。

* **ファイル／コード数の肥大化**: 前述の通りファイル数・行数が非常に多く、小さなコンペに対してオーバーエンジニアリング気味です。**「シンプルな構造で十分なところを、不必要に複雑化している」**点が見受けられます。

### コード品質と実装上の問題

* **ターゲットエンコーディング実装の不備**: 例えばスクリプト`scripts/s5e7_advanced_workflow.py`の中で、本来クロスバリデーション内で行うべきターゲットエンコーディングを全体で一括処理している箇所がありました（foldごとの分割を無視している）ため、データリークのリスクがあります。一般に**エンコーディング処理は各CV fold内で学習データの情報のみに基づいて行う必要**があります。この実装ミスによりバリデーションスコアが過剰に楽観的になっていた可能性があります。

* **不統一な前処理パイプライン**: モデル間で処理フローが統一されていません。例えばCatBoostモデルだけ他と異なる前処理経路を辿るなど、一貫性の欠如した設計になっています。これではモデルごとの比較やアンサンブル時の整合性チェックが難しく、バグの温床になりかねません。

* **ハードコーディングの散在**: 各所に埋め込まれたマジックナンバーやパラメータ値があり、調整すべき設定がコードに直書きされています。これでは再現性や設定変更の容易性が損なわれます。

### モデル性能・検証上の課題

* **過学習のリスク**: 特徴量エンジニアリングが非常に複雑で大量の特徴量を生成していますが、その中には効果の薄いものも含まれており、ノイズによる過学習を招く恐れがあります。モデルがデータの本質ではなく個別のノイズに適合してしまい、汎化性能が下がるリスクがあります。

* **検証戦略の問題**: 前述のターゲットエンコーディングの不備に代表されるように、現在のクロスバリデーションに情報漏洩のリスクが存在します。検証スキームが不適切だと、公開LBスコアは良くても非公開LB（最終評価）で大きく順位が下がる「Shake-up」現象につながりかねません。

* **モデル選択の非効率**: 現在6種類ものモデルバリエーションを試していますが、それぞれの最適化が不十分です。多数のモデルを扱っているもののチューニングに十分な時間を割けず、結果としてどれも本領を発揮できていない状況です。むしろ少数の有力なモデルに絞り込み、それらを高度に最適化する方が効果的と考えられます。

以上の問題点を踏まえ、次章で新しいプロジェクト構造と改善策を提案します。

## 新しいプロジェクト構造と基盤システム

現行の問題を解決するため、プロジェクトの骨格を**ゼロベースで再設計**します。以下に提案するディレクトリ構成および基盤システムは、シンプルさと拡張性のバランスを意識しています（プロジェクト名を仮に`solid-ml-stack-s5e7`とします）。

```plaintext
solid-ml-stack-s5e7/
├── .env                  # 環境変数の定義ファイル（APIキー等）
├── .env.example          # 環境変数のテンプレート（共有用）
├── .gitignore            # Git管理除外設定
├── pyproject.toml        # プロジェクトメタデータ・依存関係定義
├── Makefile              # 開発・実行コマンド定義
├── README.md             # プロジェクト概要ドキュメント
│
├── config/
│   ├── settings.py       # 設定管理（環境変数の読み込み）
│   └── constants.py      # 全体で使う定数定義
│
├── src/
│   ├── data.py           # データ読み込み・保存（DuckDB利用）
│   ├── features.py       # 特徴量生成・前処理ロジック
│   ├── models.py         # モデル定義・学習処理（LGBM, XGB, CatBoost等）
│   ├── validation.py     # クロスバリデーションと評価指標計算
│   ├── ensemble.py       # アンサンブル手法実装（ブレンディング等）
│   ├── submission.py     # 提出用ファイル作成
│   └── utils/
│       ├── timing.py     # 実行時間計測・記録ユーティリティ
│       ├── webhook.py    # Webhook通知ユーティリティ
│       └── logging.py    # ログ設定ユーティリティ
│
├── data/
│   ├── kaggle_data.duckdb   # DuckDBデータベースファイル（ローカル設置）
│   └── execution_log.json   # 実行時間ログ（各処理の所要時間を保存）
│
├── tests/
│   ├── test_data.py       # data.pyのユニットテスト
│   ├── test_features.py   # features.pyのユニットテスト
│   ├── test_models.py     # models.pyのユニットテスト
│   └── conftest.py        # pytest設定
│
├── scripts/
│   ├── setup_data.py      # データ初期取り込みスクリプト
│   ├── train.py           # モデル学習実行スクリプト
│   └── predict.py         # 推論・提出ファイル生成スクリプト
│
└── outputs/
    ├── models/            # 学習済みモデル保存用
    ├── submissions/       # 提出ファイル保存用
    └── logs/              # 実行ログ保存用
```

**構造のポイント:**

* シンプルで理解しやすいレイアウト：役割ごとにディレクトリ/ファイルを整理し、**単一責任の原則**に沿ってモジュールを配置しています（例えば、データ処理は`data.py`、特徴量関連は`features.py`に集約）。大規模なクラス設計を避け、関数ベースで分かりやすく実装します。

* **設定管理の一元化**: `config/settings.py`では、Pydanticの`BaseSettings`を用いて環境変数から設定を読み込みます。APIキーやパス等の機密情報はプロジェクトルートの`.env`ファイルで管理し、コード上に直接書かない設計です。例えば`.env`には以下のような情報を記載します：

  ```dotenv
  KAGGLE_USERNAME=＜Kaggleユーザー名＞
  KAGGLE_KEY=＜Kaggle APIキー＞
  WEBHOOK_URL=https://discord.com/api/webhooks/...   # Discord通知先（任意）
  LOG_LEVEL=INFO
  DATABASE_PATH=./data/kaggle_data.duckdb
  ```

  そして`settings.py`でこれらを環境変数として読み込み、プロジェクト全体で`Settings`インスタンス経由で参照できるようにします。これにより**設定値の集約と安全な管理**が可能となり、環境ごとの切り替えも容易です。

* **実行時間の記録と予測**: `src/utils/timing.py`に実行タイマーを実装し、各主要処理（前処理・学習・推論など）の開始・終了時刻を計測して`data/execution_log.json`に蓄積します。さらに過去ログから平均所要時間を算出し、処理開始時に**「予想終了時刻」を自動表示**する機能も持たせます。例えば学習を開始する際、過去の平均から**「予想終了時刻: 21:45:30」**のようにコンソール出力し、長時間処理でも終了見込みを掴めるようにします。これらにより**時間管理と見積もり精度の向上**が期待できます。

* **Webhook通知による進捗把握**: `src/utils/webhook.py`では、Discord等のWebhook URLを設定することで、モデル学習完了や予測完了時に自動で通知メッセージを送信できるようにします。例えば学習完了時には所要時間や評価スコアを含むメッセージを送る実装とし、遠隔でも結果を即座に把握可能です。チーム開発の場合、メンバーへ即時共有する用途にも役立ちます（Webhookは`.env`でURLが設定された場合のみ動作するオプショナル機能です）。

* **コード品質と再現性の確保**: 開発環境とCIを整備し、いくつかの仕組みで品質を担保します。

  * 依存パッケージは`pyproject.toml`で明示し、開発用には`pytest`, `black`（自動整形）, `flake8`（リント）, `mypy`（型チェック）, `pre-commit`等を導入します。フォーマッタやリントの設定も適切に行い、コードスタイルの一貫性とバグ検出を強化します。

  * ユニットテストを`tests/`ディレクトリに用意し、データ処理・特徴量生成・モデル学習それぞれについて**最低限の検証**を行います。特にデータ漏洩が起きていないか、意図した変換になっているかをテストで確認します。

  * **`Makefile`**で開発コマンドを定義し、例えば`make setup`で環境構築とデータ取り込み、`make test`でテスト実行、`make train`でモデル学習...といった操作をコマンド一つで再現できます。これにより**誰でも同じ手順でプロジェクトを動かせる再現性**と、頻出タスクの自動化による効率化を図ります。

以上が基盤システムの概要です。この新構成により、設定・データ・コードの見通しが大幅に改善され、プロジェクトの拡張や調整が容易になることが期待できます。

## データ管理とパイプライン戦略

機械学習コンペにおいては、**データの前処理パイプラインを明確に分離し管理すること**が再現性と性能向上の鍵となります。本提案では以下のデータ管理戦略を採用します。

* **DuckDBによるローカルデータベース管理**: すべてのデータ（生データから特徴量テーブルまで）をローカルのDuckDBデータベースファイル（`data/kaggle_data.duckdb`）で一元管理します。CSVやメモリ上のPandasだけでなく、DuckDBを使うことで**SQLライクな操作と高速クエリ**が可能になり、大規模データでも効率的に処理できます。また外部DBサーバ不要でプロジェクトディレクトリ内で完結するため、移植性・ポータブル性も高まります（他の環境へリポジトリごと持って行くだけで動作可能）。

* **データのステージ分離（スキーマ設計）**: データ処理の各段階を明確に分け、DuckDB内でスキーマやテーブルを層別管理します。具体的には以下の3段階に分類します:

  1. **Raw（生データ）**: コンペ提供の訓練データ・テストデータそのまま（例: `train_raw`テーブル, `test_raw`テーブル）。ここでは外部データ取り込みや結合も行わず、与えられたデータセットをそのまま格納します。

  2. **Preprocessed（前処理済みデータ）**: 生データに対し、欠損値補完・異常値処理・基本的なクレンジングを実施したデータ（例: `train_preprocessed`, `test_preprocessed`テーブル）。モデルに投入する前段階として、ノイズ削減や型変換などを行います。

  3. **Engineered（特徴量加工データ）**: 前処理済みデータから特徴量エンジニアリングを施し、新たな変数を追加した最終学習用データ（例: `train_engineered`, `test_engineered`テーブル）。ここにはターゲットエンコーディングや統計量集計、新特徴量の生成結果が含まれます。

  こうした段階的スキーマにより、**各処理ステップの成果物をデータベース内に蓄積**できます。他のKaggleパイプラインでも、生データ・加工データ・特徴量データを分けて保存する手法は一般的であり、途中結果を再利用したり検証したりするのに役立ちます。

* **パイプライン管理とデータ品質チェック**: データの各ステージ遷移時に、以下の点を自動チェック・ログ化します。

  * レコード数・統計量の変化: 前段階から行数が変わっていないか、欠損や外れ値の数に大きな差異がないかを記録します。
  * 分布の変化: 主要な特徴について、加工前後で分布が大きく変わりすぎていないか（例: 正規化や外れ値除去で分布が適切になるか）を確認します。
  * 処理履歴の記録: どのスクリプト/関数でどんな変換を行ったかメタデータを残します（変換内容や日時を`execution_log.json`等に追記）。
  * **増分更新**: データ追加や変更があった場合に、初めから全処理をやり直さず変更部分のみ更新できるよう、処理を関数化・モジュール化しておきます。例えば、新たな外部データソースを追加する際はRaw層にテーブル追加→Preprocessed/Engineeredで結合処理、というように**変更箇所を限定**します。

* **データ戦略の効果**:

  * **再現性向上**: 前処理→特徴量生成の各段階を明確に分離することで、同じ手順を踏めば常に同じデータセットが得られる状態にします。コンペ終了後でも**データ処理パイプラインをそのまま再実行可能**となり、結果の再現性が担保されます。

  * **デバッグ容易性**: 例えばスコア悪化や不具合が生じた場合でも、Raw/Preprocessed/Engineeredの各段階でデータをチェックすることで、問題箇所を素早く特定できます。特にリークや過学習につながる不適切な特徴量が無いか、Engineered層のデータを直接検証できます。

  * **開発効率の向上**: 一度PreprocessedやEngineeredを作成すれば、以降の実験ではそれらを**キャッシュ**として再利用可能です。重い前処理を毎回やり直す必要がなくなり、新しいモデルの試行や特徴量追加のサイクルを高速化できます。

  * **チーム協調**: チームメンバー間でデータの状態を共有しやすくなります。全員が同じDuckDBファイル・同じテーブル群を扱うことで、「誰がどのデータを使っているか」不明になる事態を避けられます。また、データのバージョン管理も容易になり、余計な重複ファイル（異なる前処理をしたCSVが散乱する等）を防げます。

以上のように、データ管理基盤を整えることでモデル開発の土台を強固にし、後工程のモデリングやチューニングをスムーズに進められるようにします。

## シンプルなアーキテクチャ設計の提案

前述の新構造・データ基盤を踏まえ、**よりシンプルで理解しやすいアーキテクチャ**を採用することが肝要です。具体的には以下の設計原則と方針に従います。

* **単一責任の原則とモジュール化**: 各モジュール（ファイル）は一つの責務に集中させます。例えば`features.py`は特徴量生成の関数群のみを持ち、モデル学習ロジックは持ちません。こうすることでモジュール間の依存を減らし、問題発生時の切り分けが容易になります。大規模なクラス継承よりも関数やシンプルなクラスの組み合わせで実装し、「必要十分」な設計に留めます。Kaggle上位勢の公開コードを見ても、似たようなアプローチで機能ごとにシンプルにファイルを分けている例が多く見られます。

* **設定管理の簡素化**: 複雑な設定フレームワークは避け、必要最小限のパラメータを`config`モジュールやYAML/JSONで管理します。本プロジェクトでは環境変数＋Pydantic設定クラスで対応しますが、それ以上に設定項目が増えそうな場合は`config.yaml`にまとめて読み込むなど、扱いやすい形式を検討します。要は**「設定値は一箇所から変更でき、コード中に埋め込まない」**という方針を徹底します。

* **統一された検証戦略**: すべてのモデルで**同じクロスバリデーション戦略**を適用し、比較可能なスコアを得ます。例えば分類タスクであれば`StratifiedKFold(n_splits=k, shuffle=True, random_state=42)`のように層化抽出＋乱数シード固定を全モデル共通で使います。これにより、モデル間の性能差異が適切に評価できるだけでなく、再現性も確保されます。また、**ターゲットエンコーディングや欠損値補完といった前処理もCVループ内で実行**し、一切の情報漏洩を排除します。

* **段階的な機能拡張**: 新しいアーキテクチャではまず**シンプルなベースラインモデル**からスタートし、その結果を軸に段階的に改善を施す方針を取ります。いきなり複雑な特徴量や多数モデルを投入するのではなく、

  1. **Baseline**: 初期ベースラインモデルの構築（後述）
  2. **Features**: 有効な特徴量の追加・改善
  3. **Models**: モデルの高度化・最適化（後述）
  4. **Ensemble**: 複数モデルの融合

  の順に、実験結果を確認しながら一歩ずつ強化します。この段階的アプローチにより、**どの要素がスコア向上に寄与したか**を把握しやすく、闇雲な改善による過学習を防ぎます。

以上の原則に沿ってアーキテクチャを簡素化することで、開発者・モデル双方にとって無理のない構成となり、結果的に精度向上と安定性に繋がると期待できます。

## 改善のロードマップと具体的ステップ

次に、上記の新アーキテクチャのもとで**Kaggle銅メダル相当のスコア**を達成するための具体的なアクションプランを示します。以下のステップを順に実行・検証していきます。

1. **ベースラインモデルの確立**: まずはごく基本的なモデルでベースラインスコアを測定します。例えば**LightGBM**を用い、現状の主要特徴量のみでモデル学習を行います。特徴量エンジニアリングは最小限に留め、複雑な加工は一切せずにスコア（クロスバリデーション平均）を算出します。目標とする評価指標で**0.975前後**のスコアをまず達成し、これを今後の比較基準とします。ベースラインがしっかりしていれば、以降の改善の有効性を正しく判断できます。

2. **特徴量エンジニアリングの洗練**: 現行プロジェクトで作成されている大量の特徴量を見直し、**効果の高い特徴量に絞り込み**ます。具体的には各特徴量の単変量での情報量（ターゲットとの相関や分布差異）を分析し、寄与の低い特徴量や重複しているものを削減します。また、新たな特徴量候補がある場合はドメイン知識（後述の心理学的特徴など）に基づき追加し、**一度に大量ではなく少数ずつ**CVスコアへの影響を確認しながら導入します。ターゲットエンコーディング等は**リーク対策を施しつつ（各fold内で計算）**慎重に実施します。この段階では特徴量の質を高め、**過剰な次元拡張による過学習を防ぐ**ことに注力します。

3. **クロスバリデーションの安定化**: 改善されたデータ・特徴量を用いて改めてCV戦略を精査します。分布やデータ特性に応じて適切なK-fold数や方法（層化、GroupKFold等）を選択し、**乱数シードを固定**してブレの少ない検証を行います。もしデータ中に時間的要因やグループ（例: 同一人物の複数レコード）がある場合は、漏洩しないよう**グループ単位でfoldを分ける**などの対策もここで講じます。CVスコアが信頼できるものであれば、公開LBに過度に依存せずとも最終順位を予測できます。「**Trust Your CV**（自分のCVを信じろ）」というKaggle競技の鉄則があるように、ここで堅牢な検証を確立することが銅メダル獲得への近道です。

4. **モデルのハイパーパラメータ最適化**: ベースラインモデルで安定したCVが得られたら、ハイパーパラメータチューニングを行います。Optunaなどの自動最適化ツールを用いて**LightGBM**を中心にパラメータ探索し、スコアを底上げします。その後、**XGBoost**や**CatBoost**といった他の勾配ブースティングモデルも試し、それぞれ最適化を行います。モデルごとに異なる長所（高速な学習、外れ値耐性、カテゴリカル変数の扱いやすさ等）があるため、複数モデルを比較検討しつつ、**上位モデルを2～3種類**選定します。ここではモデル数を増やしすぎないことがポイントです。少数精鋭のモデルに絞ることで各モデルに十分なチューニング時間を割け、結果的に精度向上につながります。

5. **アンサンブルの構築**: 最終局面では選定した複数モデルのアンサンブルを行います。手法としては過度に複雑なスタッキングよりも、まずは**単純な重み付き平均**によるブレンディングでモデル予測値を融合します。各モデルのCVスコアを重み設定の目安にしつつ、Validationデータでの組み合わせ性能を確認して重みを調整します。必要に応じて、Out-of-Fold予測を用いたメタモデル（例えばロジスティック回帰）によるスタッキングも検討しますが、まずはシンプルな手法で十分な性能が出るか確認します。アンサンブルによって**個別モデルの弱点が相互補完され、安定したスコア向上**が見込めます。

以上のステップを踏むことで、段階ごとに着実にモデルの精度と信頼性を高めていきます。常にCVスコアを指標に改善を判断し、ブレの少ない開発を心がけます。

## Kaggle銅メダル獲得に向けた追加戦略

上記プランに加え、Kaggleメダル圏に入るために有効と思われる戦略・工夫を補足します。

* **ドメイン知識の活用**: データが「内向/外向の性格予測」という趣旨であれば、心理学の知見に基づく特徴量エンジニアリングが有効かもしれません。例えばアンケート項目同士の相関やビッグファイブ性格指標のような**高次の組み合わせ特徴**を検討します。ただし、これらも闇雲に追加すると過学習のもとになるため、**有意義な組み合わせを論理的に選定**し順次試すアプローチを取ります。ドメイン知識由来の新特徴がCVで有効性を示せれば大きなアドバンテージになります。

* **特徴量とターゲットの関係精査（リークチェック）**: 特徴量の中にターゲットと強い相関を持つものが存在しないか、あるいは将来予測に使えない情報が混入していないか、改めて確認します。例えば時系列データなら未来情報の漏洩がないか、IDや分布の偏りによる疑似相関がないか等です。特にターゲットエンコーディングや統計系特徴では、**データ分割に合わせて計算しているか**を再チェックします（前述のCV内処理徹底）。データ分析段階で不自然な高相関の特徴があれば除外や修正を検討し、**モデルが本質的なパターンを学習するよう**誘導します。

* **Shake-up対策と汎化性能の重視**: コンペ終盤では公開リーダーボード上位に合わせ込みたくなりますが、最終評価は非公開データで行われるため、公開LBスコアへの過度な最適化は危険です。むしろ**「CVスコアを信頼し、自分の検証で一貫して良いモデル」を最終モデルとする**ことが重要です。このため、アンサンブルも多様性より**再現性と安定性**を優先し、外れ値的なモデルを含めないようにします。また、データ分布の変化に強いモデルを選ぶ・正則化を強めにかける・アーリーストッピングで過学習を防ぐ等、**汎化性能を高める調整**を行います。これらにより、大きなShake-upが起きてもメダル圏を維持できる可能性が高まります。

* **その他の微調整**: 上記以外にも、例えば:

  * モデルの出力の**キャリブレーション**（必要ならばPlatt scalingや温度スケーリングで確率の信頼性を向上）
  * **外れ値の検出と対処**: 明らかに誤ったデータがあれば除外検討
  * **アンサンブルの最終チェック**: 外部データや異なる手法（例えば簡単なニューラルネット）の結果も組み合わせられないか検討

  といった細かな工夫も状況に応じて検討します。ただし、大局に影響しない微調整に時間を割きすぎず、**基本に忠実なアプローチ**を徹底することが結果的に良い成果につながります。

## 結論と推奨事項

以上、プロジェクトの現状課題と新たな設計・改善策を包括的に述べました。最大の提言は、**現在の複雑化しすぎたコードベースに固執せず**、得られた知見を活かしつつ**新しいシンプルな構造で再実装すること**です。その際、本書で示したようなプロジェクトインフラ（設定管理・ログ・データパイプライン）を整備することで、開発効率とモデル精度の双方が向上します。実装にあたっては段階的に検証を行い、常に論理的なエビデンス（CVスコアや統計検定）に基づいて意思決定する方針を貫いてください。

最終目標である**Kaggle銅メダル**に向け、本提案がプロジェクト再構築の道筋としてお役に立てば幸いです。健闘を祈ります！

**参考文献・ソース:**
* Kaggleパイプラインの構造例（TPS 2022August）: データディレクトリをraw/processed/featuresに分離、モジュール構成
* データ漏洩防止の一般知見: エンコーディング処理はクロスバリデーション内で実行
* Kaggle攻略の心得: **CVを信頼し過度なLB合わせ込みを避ける**（公開・非公開スコア差への対策）
