"""
Bronze Level Data Management
Raw Data Standardization & Quality Assurance (Entry Point to Medallion Pipeline)
"""

from typing import Tuple, Dict, Any, Optional
import warnings

import duckdb
import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.ensemble import IsolationForest

DB_PATH = "/home/wsl/dev/my-study/ml/solid-ml-stack-s5e7/data/kaggle_datasets.duckdb"

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)


def load_data() -> Tuple[pd.DataFrame, pd.DataFrame]:
    """Raw data access point - Single source entry to Medallion pipeline"""
    conn = duckdb.connect(DB_PATH)
    train = conn.execute("SELECT * FROM playground_series_s5e7.train").df()
    test = conn.execute("SELECT * FROM playground_series_s5e7.test").df()
    conn.close()
    
    # Explicit dtype setting for LightGBM optimization
    train = _set_optimal_dtypes(train)
    test = _set_optimal_dtypes(test)
    
    return train, test


def _set_optimal_dtypes(df: pd.DataFrame) -> pd.DataFrame:
    """Set optimal dtypes for LightGBM compatibility and performance"""
    df = df.copy()
    
    # Numeric features - use float32 for memory efficiency when possible
    numeric_cols = ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 
                   'Friends_circle_size', 'Post_frequency']
    
    for col in numeric_cols:
        if col in df.columns:
            # Check if values fit in float32 range
            if df[col].dtype in ['float64', 'float32']:
                col_min, col_max = df[col].min(), df[col].max()
                if col_min >= -3.4e38 and col_max <= 3.4e38:
                    df[col] = df[col].astype('float32')
                else:
                    df[col] = df[col].astype('float64')
            elif df[col].dtype in ['int64', 'int32']:
                # Convert integers to float for LightGBM
                df[col] = df[col].astype('float32')
    
    # Categorical features - ensure object type for processing
    categorical_cols = ['Stage_fear', 'Drained_after_socializing']
    for col in categorical_cols:
        if col in df.columns:
            # Convert to string first, then to object
            df[col] = df[col].astype(str).astype('object')
    
    return df


def validate_data_quality(df: pd.DataFrame) -> Dict[str, Any]:
    """Type validation and range guards for data quality assurance"""
    validation_results = {
        'type_validation': {},
        'range_validation': {},
        'schema_validation': {},
        'quality_metrics': {}
    }
    
    # Type validation
    expected_numeric = ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 
                       'Friends_circle_size', 'Post_frequency']
    expected_categorical = ['Stage_fear', 'Drained_after_socializing']
    
    for col in expected_numeric:
        if col in df.columns:
            validation_results['type_validation'][col] = pd.api.types.is_numeric_dtype(df[col])
    
    for col in expected_categorical:
        if col in df.columns:
            validation_results['type_validation'][col] = df[col].dtype == 'object'
    
    # Range validation with more comprehensive checks
    if 'Time_spent_Alone' in df.columns:
        # 数値型かどうかをチェックしてから比較
        if pd.api.types.is_numeric_dtype(df['Time_spent_Alone']):
            validation_results['range_validation']['Time_spent_Alone'] = {
                'within_24hrs': (df['Time_spent_Alone'] <= 24).all(),
                'non_negative': (df['Time_spent_Alone'] >= 0).all(),
                'finite_values': np.isfinite(df['Time_spent_Alone']).all()
            }
        else:
            validation_results['range_validation']['Time_spent_Alone'] = {
                'within_24hrs': False,
                'non_negative': False,
                'finite_values': False
            }
    
    for col in ['Social_event_attendance', 'Going_outside', 'Friends_circle_size', 'Post_frequency']:
        if col in df.columns:
            if pd.api.types.is_numeric_dtype(df[col]):
                validation_results['range_validation'][col] = {
                    'non_negative': (df[col] >= 0).all(),
                    'finite_values': np.isfinite(df[col]).all()
                }
            else:
                validation_results['range_validation'][col] = {
                    'non_negative': False,
                    'finite_values': False
                }
    
    # Quality metrics
    validation_results['quality_metrics'] = {
        'total_rows': len(df),
        'total_columns': len(df.columns),
        'missing_values': df.isnull().sum().sum(),
        'duplicate_rows': df.duplicated().sum()
    }
    
    return validation_results


def encode_categorical_robust(df: pd.DataFrame) -> pd.DataFrame:
    """Yes/No normalization with case-insensitive unified mapping → {0,1}"""
    df = df.copy()
    
    categorical_columns = ['Stage_fear', 'Drained_after_socializing']
    
    for col in categorical_columns:
        if col in df.columns:
            # Handle NaN values first
            df[col] = df[col].fillna('Unknown')
            
            # Case-insensitive Yes/No → 1/0 mapping
            df[col] = df[col].astype(str).str.lower().str.strip()
            
            # More robust mapping
            yes_values = ['yes', 'y', '1', 'true']
            no_values = ['no', 'n', '0', 'false']
            
            # Create mapping dictionary
            mapping_dict = {}
            for yes_val in yes_values:
                mapping_dict[yes_val] = 1.0
            for no_val in no_values:
                mapping_dict[no_val] = 0.0
            
            # Apply mapping, keep NaN for unknown values (LightGBM will handle)
            df[col] = df[col].map(mapping_dict)
            
            # Convert to float64 for LightGBM compatibility
            df[col] = df[col].astype('float64')
            
            # Create encoded column for compatibility
            encoded_col = f"{col}_encoded"
            df[encoded_col] = df[col]
    
    return df


def advanced_missing_pattern_analysis(df: pd.DataFrame) -> pd.DataFrame:
    """
    高度な欠損パターン分析と条件付き欠損フラグ生成
    
    Args:
        df: 入力データフレーム
        
    Returns:
        欠損パターンフラグが追加されたデータフレーム
        
    Expected Impact: +0.3-0.5%
    """
    df = df.copy()
    
    # 1. 基本的な欠損フラグ（既存実装の拡張）
    high_impact_features = [
        'Stage_fear', 'Drained_after_socializing', 'Going_outside',
        'Time_spent_Alone', 'Social_event_attendance', 'Friends_circle_size'
    ]
    
    for col in high_impact_features:
        if col in df.columns:
            missing_flag_col = f"{col}_missing"
            df[missing_flag_col] = df[col].isna().astype('int32')
    
    # 2. 条件付き欠損フラグ（高相関パターン）
    if 'Stage_fear' in df.columns and 'Drained_after_socializing' in df.columns:
        # 両方同時欠損（社会的不安の完全回避パターン）
        df['social_anxiety_complete_missing'] = (
            df['Stage_fear'].isna() & df['Drained_after_socializing'].isna()
        ).astype('int32')
        
        # 片方のみ欠損（部分的回避パターン）
        df['social_anxiety_partial_missing'] = (
            df['Stage_fear'].isna() ^ df['Drained_after_socializing'].isna()  # XOR
        ).astype('int32')
        
        # 社会的疲労関連欠損（内向性指標）
        if 'Social_event_attendance' in df.columns:
            df['social_fatigue_missing'] = (
                df['Drained_after_socializing'].isna() & 
                (df['Social_event_attendance'] > df['Social_event_attendance'].quantile(0.7))
            ).astype('int32')
    
    # 3. 行動パターン関連欠損
    if 'Time_spent_Alone' in df.columns and 'Social_event_attendance' in df.columns:
        # 高孤独時間 + ソーシャル欠損（極端な内向性）
        df['extreme_introvert_missing'] = (
            (df['Time_spent_Alone'] > df['Time_spent_Alone'].quantile(0.8)) & 
            df['Social_event_attendance'].isna()
        ).astype('int32')
        
        # 低孤独時間 + ソーシャル欠損（矛盾パターン）
        df['contradictory_social_missing'] = (
            (df['Time_spent_Alone'] < df['Time_spent_Alone'].quantile(0.2)) & 
            df['Social_event_attendance'].isna()
        ).astype('int32')
    
    # 4. 外出パターン関連欠損
    if 'Going_outside' in df.columns and 'Social_event_attendance' in df.columns:
        # 高外出頻度 + ソーシャルイベント欠損（非ソーシャル外出）
        df['non_social_outing_missing'] = (
            (df['Going_outside'] > df['Going_outside'].quantile(0.7)) & 
            df['Social_event_attendance'].isna()
        ).astype('int32')
    
    # 5. コミュニケーション関連欠損
    if 'Post_frequency' in df.columns and 'Friends_circle_size' in df.columns:
        # 高投稿頻度 + 友達数欠損（オンライン社交性）
        df['online_social_missing'] = (
            (df['Post_frequency'] > df['Post_frequency'].quantile(0.7)) & 
            df['Friends_circle_size'].isna()
        ).astype('int32')
    
    return df


def cross_feature_imputation(df: pd.DataFrame) -> pd.DataFrame:
    """
    クロス特徴量補完戦略
    
    Args:
        df: 入力データフレーム
        
    Returns:
        補完が適用されたデータフレーム
        
    Expected Impact: +0.2-0.4%
    """
    df = df.copy()
    
    # 1. Stage_fear ↔ Drained_after_socializing 相関ベース補完
    if 'Stage_fear' in df.columns and 'Drained_after_socializing' in df.columns:
        # 両方の特徴量が存在する場合の相関を計算
        valid_mask = df['Stage_fear'].notna() & df['Drained_after_socializing'].notna()
        if valid_mask.sum() > 10:  # 十分なサンプルがある場合
            correlation = df.loc[valid_mask, ['Stage_fear', 'Drained_after_socializing']].corr().iloc[0, 1]
            
            if abs(correlation) > 0.3:  # 相関が高い場合のみ補完
                # Stage_fearが欠損でDrained_after_socializingが存在する場合
                stage_missing = df['Stage_fear'].isna() & df['Drained_after_socializing'].notna()
                if stage_missing.any():
                    # 相関に基づいて推定
                    if correlation > 0:
                        df.loc[stage_missing, 'Stage_fear'] = df.loc[stage_missing, 'Drained_after_socializing']
                    else:
                        df.loc[stage_missing, 'Stage_fear'] = 1 - df.loc[stage_missing, 'Drained_after_socializing']
                
                # Drained_after_socializingが欠損でStage_fearが存在する場合
                drained_missing = df['Drained_after_socializing'].isna() & df['Stage_fear'].notna()
                if drained_missing.any():
                    if correlation > 0:
                        df.loc[drained_missing, 'Drained_after_socializing'] = df.loc[drained_missing, 'Stage_fear']
                    else:
                        df.loc[drained_missing, 'Drained_after_socializing'] = 1 - df.loc[drained_missing, 'Stage_fear']
    
    # 2. 行動パターンベース補完
    if 'Going_outside' in df.columns and 'Social_event_attendance' in df.columns:
        # 高外出頻度 → ソーシャルイベント参加推定
        high_outing = df['Going_outside'] > df['Going_outside'].quantile(0.7)
        social_missing_outing = df['Social_event_attendance'].isna() & high_outing
        
        if social_missing_outing.any():
            # 高外出頻度の人のソーシャルイベント参加率を推定
            high_social_value = df.loc[high_outing & ~df['Social_event_attendance'].isna(), 'Social_event_attendance'].quantile(0.75)
            df.loc[social_missing_outing, 'Social_event_attendance'] = high_social_value
    
    # 3. 時間配分ベース補完
    if 'Time_spent_Alone' in df.columns and 'Going_outside' in df.columns:
        # 低孤独時間 → 高外出頻度推定
        low_alone = df['Time_spent_Alone'] < df['Time_spent_Alone'].quantile(0.2)
        going_missing_alone = df['Going_outside'].isna() & low_alone
        
        if going_missing_alone.any():
            high_going_value = df.loc[low_alone & ~df['Going_outside'].isna(), 'Going_outside'].quantile(0.75)
            df.loc[going_missing_alone, 'Going_outside'] = high_going_value
    
    # 4. 友達数ベース補完
    if 'Friends_circle_size' in df.columns and 'Social_event_attendance' in df.columns:
        # 友達数からソーシャル活動を推定
        friends_high = df['Friends_circle_size'] > df['Friends_circle_size'].quantile(0.7)
        social_missing_friends = df['Social_event_attendance'].isna() & friends_high
        
        if social_missing_friends.any():
            high_social_value = df.loc[friends_high & ~df['Social_event_attendance'].isna(), 'Social_event_attendance'].quantile(0.75)
            df.loc[social_missing_friends, 'Social_event_attendance'] = high_social_value
    
    # 5. 投稿頻度ベース補完
    if 'Post_frequency' in df.columns and 'Friends_circle_size' in df.columns:
        # 高投稿頻度 → 高友達数（オンライン社交性）
        post_high = df['Post_frequency'] > df['Post_frequency'].quantile(0.7)
        friends_missing_post = df['Friends_circle_size'].isna() & post_high
        
        if friends_missing_post.any():
            high_friends_value = df.loc[post_high & ~df['Friends_circle_size'].isna(), 'Friends_circle_size'].quantile(0.75)
            df.loc[friends_missing_post, 'Friends_circle_size'] = high_friends_value
    
    return df


def advanced_outlier_detection(df: pd.DataFrame) -> pd.DataFrame:
    """
    高度な異常値検出と処理
    
    Args:
        df: 入力データフレーム
        
    Returns:
        異常値フラグが追加されたデータフレーム
        
    Expected Impact: +0.2-0.3%
    """
    df = df.copy()
    
    # 対象となる数値特徴量
    numeric_cols = [
        'Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 
        'Friends_circle_size', 'Post_frequency'
    ]
    
    # 1. Isolation Forest による異常値検出
    try:
        # 数値データのみでIsolation Forest実行
        available_numeric = [col for col in numeric_cols if col in df.columns]
        if len(available_numeric) >= 2:  # 最低2つの特徴量が必要
            numeric_data = df[available_numeric].fillna(df[available_numeric].median())
            
            # パラメータ調整（contamination=0.05で5%を異常値として検出）
            iso_forest = IsolationForest(
                contamination=0.05,  # 5%を異常値として検出
                random_state=42,
                n_estimators=100,
                max_samples='auto'
            )
            
            # 異常値スコアの計算
            outlier_scores = iso_forest.fit_predict(numeric_data)
            
            # 異常値フラグ（-1が異常値）
            df['isolation_forest_outlier'] = (outlier_scores == -1).astype('int32')
            
            # 異常値スコアの保存（連続値）
            df['isolation_forest_score'] = iso_forest.decision_function(numeric_data)
            
        else:
            df['isolation_forest_outlier'] = 0
            df['isolation_forest_score'] = 0
            
    except Exception as e:
        print(f"Isolation Forest failed: {e}")
        df['isolation_forest_outlier'] = 0
        df['isolation_forest_score'] = 0
    
    # 2. 統計的異常値検出（改良版IQR）
    for col in numeric_cols:
        if col in df.columns and df[col].notna().sum() > 10:
            # 基本統計量の計算
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            
            # より厳密な境界設定（従来の1.5 → 2.5）
            lower_bound = Q1 - 2.5 * IQR
            upper_bound = Q3 + 2.5 * IQR
            
            # 異常値フラグ
            outlier_flag = f"{col}_statistical_outlier"
            df[outlier_flag] = ((df[col] < lower_bound) | (df[col] > upper_bound)).astype('int32')
            
            # 異常値スコア（境界からの距離）
            df[f"{col}_outlier_score"] = np.where(
                df[col] < lower_bound,
                (lower_bound - df[col]) / IQR,
                np.where(
                    df[col] > upper_bound,
                    (df[col] - upper_bound) / IQR,
                    0
                )
            )
    
    # 3. Z-score ベース異常値検出
    for col in numeric_cols:
        if col in df.columns and df[col].notna().sum() > 10:
            # Z-score計算
            mean_val = df[col].mean()
            std_val = df[col].std()
            
            if std_val > 0:
                z_scores = np.abs((df[col] - mean_val) / std_val)
                
                # Z-score > 3 を異常値として検出
                z_outlier_flag = f"{col}_zscore_outlier"
                df[z_outlier_flag] = (z_scores > 3).astype('int32')
                
                # Z-score値の保存
                df[f"{col}_zscore"] = z_scores
    
    # 4. 複合異常値フラグ
    outlier_flags = [col for col in df.columns if col.endswith('_outlier')]
    if outlier_flags:
        # 複数の異常値検出手法で検出された異常値
        df['multiple_outlier_detected'] = df[outlier_flags].sum(axis=1) >= 2
        df['multiple_outlier_detected'] = df['multiple_outlier_detected'].astype('int32')
        
        # 異常値の総数
        df['total_outlier_count'] = df[outlier_flags].sum(axis=1)
    
    # 5. ドメイン特化異常値検出
    if 'Time_spent_Alone' in df.columns:
        # 24時間を超える異常値
        df['time_alone_extreme'] = (df['Time_spent_Alone'] > 24).astype('int32')
        
        # 負の値（データエラー）
        df['time_alone_negative'] = (df['Time_spent_Alone'] < 0).astype('int32')
    
    if 'Friends_circle_size' in df.columns:
        # 極端に大きな友達数（現実的でない値）
        df['friends_extreme'] = (df['Friends_circle_size'] > 1000).astype('int32')
    
    if 'Post_frequency' in df.columns:
        # 極端に高い投稿頻度
        df['post_frequency_extreme'] = (df['Post_frequency'] > 100).astype('int32')
    
    return df


def advanced_missing_strategy(df: pd.DataFrame) -> pd.DataFrame:
    """Missing value intelligence with LightGBM native handling"""
    df = df.copy()
    
    # Create missing flags for high-impact features (Winner Solution pattern)
    high_impact_features = ['Stage_fear', 'Going_outside', 'Time_spent_Alone', 
                           'Drained_after_socializing', 'Social_event_attendance']
    
    for col in high_impact_features:
        if col in df.columns:
            missing_flag_col = f"{col}_missing"
            df[missing_flag_col] = df[col].isna().astype('int32')  # LightGBM optimized
    
    # Cross-feature missing pattern analysis
    if 'Stage_fear' in df.columns and 'Drained_after_socializing' in df.columns:
        # Create interaction missing flag
        df['social_fatigue_missing'] = (
            df['Stage_fear'].isna() & df['Drained_after_socializing'].isna()
        ).astype('int32')
    
    # Preserve NaN for LightGBM native handling (don't impute)
    # LightGBM will handle NaN values automatically in tree splits
    
    return df


def winsorize_outliers(df: pd.DataFrame, percentile: float = 0.01) -> pd.DataFrame:
    """IQR-based outlier clipping for numeric stability"""
    df = df.copy()
    
    numeric_columns = ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 
                      'Friends_circle_size', 'Post_frequency']
    
    for col in numeric_columns:
        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):
            # Skip if too many NaN values
            if df[col].isna().sum() > len(df) * 0.5:
                continue
                
            # Calculate bounds using quantiles
            lower_bound = df[col].quantile(percentile)
            upper_bound = df[col].quantile(1 - percentile)
            
            # Apply clipping
            df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)
    
    return df


def enhanced_bronze_preprocessing(df: pd.DataFrame) -> pd.DataFrame:
    """
    3つの高度な前処理を統合したブロンズ層処理
    
    Args:
        df: 生データフレーム
        
    Returns:
        高度な前処理が適用されたデータフレーム
        
    Expected Total Impact: +0.7-1.2%
    """
    # 1. 高度な欠損パターン分析
    df = advanced_missing_pattern_analysis(df)
    
    # 2. クロス特徴量補完戦略
    df = cross_feature_imputation(df)
    
    # 3. 異常値検出の高度化
    df = advanced_outlier_detection(df)
    
    return df


def create_bronze_tables() -> None:
    """Creates standardized bronze.train, bronze.test tables"""
    conn = duckdb.connect(DB_PATH)
    
    # Create bronze schema
    conn.execute("CREATE SCHEMA IF NOT EXISTS bronze")
    
    # Load raw data
    train_raw, test_raw = load_data()
    
    # Apply bronze layer processing pipeline (modern robust functions)
    train_bronze = encode_categorical_robust(train_raw)
    test_bronze = encode_categorical_robust(test_raw)
    
    train_bronze = advanced_missing_strategy(train_bronze)
    test_bronze = advanced_missing_strategy(test_bronze)
    
    train_bronze = winsorize_outliers(train_bronze)
    test_bronze = winsorize_outliers(test_bronze)
    
    # Validate data quality
    train_validation = validate_data_quality(train_bronze)
    test_validation = validate_data_quality(test_bronze)
    
    # Create bronze tables
    conn.execute("DROP TABLE IF EXISTS bronze.train")
    conn.execute("DROP TABLE IF EXISTS bronze.test")
    
    conn.register("train_bronze_df", train_bronze)
    conn.register("test_bronze_df", test_bronze)
    
    conn.execute("CREATE TABLE bronze.train AS SELECT * FROM train_bronze_df")
    conn.execute("CREATE TABLE bronze.test AS SELECT * FROM test_bronze_df")
    
    print("Bronze tables created:")
    print(f"- bronze.train: {len(train_bronze)} rows, {len(train_bronze.columns)} columns")
    print(f"- bronze.test: {len(test_bronze)} rows, {len(test_bronze.columns)} columns")
    print(f"- Data quality validation: {len([k for k, v in train_validation['type_validation'].items() if v])} types passed")
    print(f"- Bronze layer features: {len(train_bronze.columns)} columns (quality assured)")
    
    conn.close()


def quick_preprocess(df: pd.DataFrame) -> pd.DataFrame:
    """Legacy preprocessing function - USE create_bronze_tables() for production pipeline"""
    df = df.copy()

    # 欠損値処理
    numeric_cols = [
        "Time_spent_Alone",
        "Social_event_attendance",
        "Going_outside",
        "Friends_circle_size",
        "Post_frequency",
    ]
    for col in numeric_cols:
        if col in df.columns:
            df[col] = df[col].fillna(df[col].median())

    # カテゴリ変換
    if "Stage_fear" in df.columns:
        df["Stage_fear_encoded"] = (df["Stage_fear"] == "Yes").astype(int)
        # 元のStage_fearを削除してLightGBM互換性を確保
        df = df.drop(columns=["Stage_fear"])
    if "Drained_after_socializing" in df.columns:
        df["Drained_after_socializing_encoded"] = (df["Drained_after_socializing"] == "Yes").astype(int)
        # 元のDrained_after_socializingを削除してLightGBM互換性を確保
        df = df.drop(columns=["Drained_after_socializing"])

    return df


def load_bronze_data() -> Tuple[pd.DataFrame, pd.DataFrame]:
    """bronze層データ読み込み"""
    conn = duckdb.connect(DB_PATH)
    train = conn.execute("SELECT * FROM bronze.train").df()
    test = conn.execute("SELECT * FROM bronze.test").df()
    conn.close()
    return train, test


# ===== Sklearn-Compatible Transformers for Pipeline Integration =====

class BronzePreprocessor(BaseEstimator, TransformerMixin):
    """Sklearn-compatible transformer for Bronze layer processing"""
    
    def __init__(self, add_features: bool = True, winsorize: bool = True):
        self.add_features = add_features
        self.winsorize = winsorize
        self.is_fitted = False
    
    def fit(self, X, y=None):
        """Fit the transformer (no fitting required for Bronze layer)"""
        self.is_fitted = True
        return self
    
    def transform(self, X):
        """Apply Bronze layer transformations"""
        if not self.is_fitted:
            raise ValueError("Transformer must be fitted before transform")
        
        # Apply Bronze pipeline
        X_transformed = encode_categorical_robust(X)
        X_transformed = advanced_missing_strategy(X_transformed)
        
        if self.winsorize:
            X_transformed = winsorize_outliers(X_transformed)
        
        return X_transformed


class FoldSafeBronzePreprocessor(BaseEstimator, TransformerMixin):
    """Fold-safe Bronze preprocessor for CV integration"""
    
    def __init__(self):
        self.categorical_mappings = {}
        self.is_fitted = False
    
    def fit(self, X, y=None):
        """Learn categorical mappings from training data only"""
        self.categorical_mappings = {}
        
        categorical_columns = ['Stage_fear', 'Drained_after_socializing']
        
        for col in categorical_columns:
            if col in X.columns:
                # Learn unique values from training data
                unique_values = X[col].dropna().unique()
                self.categorical_mappings[col] = set(unique_values)
        
        self.is_fitted = True
        return self
    
    def transform(self, X):
        """Apply fold-safe transformations"""
        if not self.is_fitted:
            raise ValueError("Transformer must be fitted before transform")
        
        X_transformed = X.copy()
        
        # Apply fold-safe categorical encoding
        categorical_columns = ['Stage_fear', 'Drained_after_socializing']
        
        for col in categorical_columns:
            if col in X_transformed.columns:
                # Handle NaN values
                X_transformed[col] = X_transformed[col].fillna('Unknown')
                
                # Case-insensitive mapping
                X_transformed[col] = X_transformed[col].astype(str).str.lower().str.strip()
                
                # Apply mapping only for known values
                yes_values = ['yes', 'y', '1', 'true']
                no_values = ['no', 'n', '0', 'false']
                
                mapping_dict = {}
                for yes_val in yes_values:
                    mapping_dict[yes_val] = 1.0
                for no_val in no_values:
                    mapping_dict[no_val] = 0.0
                
                X_transformed[col] = X_transformed[col].map(mapping_dict)
                X_transformed[col] = X_transformed[col].astype('float64')
        
        # Add missing flags
        high_impact_features = ['Stage_fear', 'Going_outside', 'Time_spent_Alone']
        for col in high_impact_features:
            if col in X_transformed.columns:
                missing_flag_col = f"{col}_missing"
                X_transformed[missing_flag_col] = X_transformed[col].isna().astype('int32')
        
        return X_transformed
