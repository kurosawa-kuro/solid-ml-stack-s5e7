# solid-ml-stack-s5e7# Solid ML Stack

Kaggle S5E7 æ€§æ ¼äºˆæ¸¬ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ç”¨ã®é«˜é€Ÿã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªæ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

## ç‰¹å¾´

  2. é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œ:
    - make model-lgb (DARTãƒ¢ãƒ¼ãƒ‰)
    - make model-cat
    - make ensemble-average
  3. æœ€é«˜æ€§èƒ½ã‚’ç›®æŒ‡ã™: python3 scripts/enhanced_ensemble_workflow.py

### ğŸ¯ Kaggle ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³æœ€é©åŒ–
- **ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–**: å‰å‡¦ç† â†’ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° â†’ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ â†’ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« â†’ æå‡ºã®è‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- **å†åˆ©ç”¨å¯èƒ½ãªè¨­è¨ˆ**: é–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ç•°ãªã‚‹ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«ã‚‚å®¹æ˜“ã«é©ç”¨å¯èƒ½
- **CPUç‰¹åŒ–**: ãƒ„ãƒªãƒ¼ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆXGBoost/LightGBM/CatBoostï¼‰ã«ã‚ˆã‚‹é«˜é€Ÿå­¦ç¿’

### ğŸ”§ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–è¨­è¨ˆ
- **å‰å‡¦ç†**: æ¬ æå€¤å‡¦ç†ã€å¤–ã‚Œå€¤é™¤å»ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- **ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**: æ•°å€¤å¤‰æ›ã€ã‚«ãƒ†ã‚´ãƒªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€äº¤äº’ä½œç”¨ç‰¹å¾´é‡ã€æ—¥æ™‚ç‰¹å¾´é‡
- **ãƒ¢ãƒ‡ãƒ«å­¦ç¿’**: XGBoostã€LightGBMã€CatBoostã€ç·šå½¢ãƒ¢ãƒ‡ãƒ«
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–**: Grid Searchã€Random Searchã€Bayesian Optimizationã€Optuna
- **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•**: å¹³å‡åŒ–ã€é‡ã¿ä»˜ãå¹³å‡ã€ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã€æŠ•ç¥¨

### ğŸ“Š ã‚¿ãƒ–ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ç‰¹åŒ–
- CSVç­‰ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å½¢å¼ã«ç‰¹åŒ–
- pandas â†’ scikit-learn ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹
- ç”»åƒãƒ»ãƒ†ã‚­ã‚¹ãƒˆãƒ»æ™‚ç³»åˆ—æ·±å±¤å­¦ç¿’ã¯å¯¾è±¡å¤–

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# åŸºæœ¬ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -e .

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³: æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -e .[optimization]

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³: å¯è¦–åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -e .[visualization]

# é–‹ç™ºãƒ„ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -e .[dev]
```

## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•

```python
import pandas as pd
from src.data.data_loader import DataLoader
from src.preprocessing.preprocessor import DataPreprocessor
from src.modeling.factory import create_kaggle_models
from src.submission.submission_generator import SubmissionGenerator

# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
loader = DataLoader()
train_data, test_data = loader.load_train_test()

# å‰å‡¦ç†
preprocessor = DataPreprocessor()
X_train = train_data.drop(['Personality', 'id'], axis=1)
y_train = train_data['Personality']
X_test = test_data.drop('id', axis=1)

X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
models = create_kaggle_models(target_type='regression')
trained_models = {}

for model in models:
    model.fit(X_train_processed, y_train)
    trained_models[model.config.name] = model

# äºˆæ¸¬ã¨æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆ
test_predictions = {}
for name, model in trained_models.items():
    test_predictions[name] = model.predict(X_test_processed)

submission_gen = SubmissionGenerator()
submission_path = submission_gen.create_submission(
    test_predictions[best_model_name], 
    test_data['id'], 
    filename='submission.csv'
)
```

### ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ

```bash
# ãƒ•ãƒ«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å®Ÿè¡Œ
python3 scripts/kaggle_workflow.py \
    --target-col Personality \
    --problem-type classification \
    --optimize \
    --ensemble

# Makefileã‚’ä½¿ç”¨ã—ãŸå®Ÿè¡Œ
make personality-prediction
make notebook-run  # çµ±åˆKaggleãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å®Ÿè¡Œ
```

### Jupyter Notebook ã®ä½¿ç”¨

```bash
# çµ±åˆKaggleæå‡ºãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®èµ·å‹•
jupyter notebook notebooks/kaggle_submission_notebook.ipynb

# ã¾ãŸã¯å€‹åˆ¥ã®åˆ†æãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å®Ÿè¡Œ
jupyter notebook notebooks/01_data_exploration_preprocessing.ipynb
jupyter notebook notebooks/02_model_training_evaluation.ipynb
jupyter notebook notebooks/03_ensemble_hyperparameter_tuning.ipynb
jupyter notebook notebooks/04_results_analysis_feature_importance.ipynb
```

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
src/
â”œâ”€â”€ analysis/               # åˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆ
â”‚   â”œâ”€â”€ comprehensive_analysis.py  # åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ
â”‚   â”œâ”€â”€ data_processor.py          # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â””â”€â”€ feature_importance.py      # ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
â”œâ”€â”€ data/                  # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ç®¡ç†
â”‚   â””â”€â”€ data_loader.py     # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”œâ”€â”€ preprocessing/         # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
â”‚   â”œâ”€â”€ preprocessor.py    # ãƒ¡ã‚¤ãƒ³å‰å‡¦ç†ã‚¯ãƒ©ã‚¹
â”‚   â”œâ”€â”€ pipeline.py        # å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
â”‚   â””â”€â”€ transformers.py    # å€‹åˆ¥å¤‰æ›å™¨
â”œâ”€â”€ features/              # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
â”‚   â””â”€â”€ engineering/       # ç‰¹å¾´é‡ç”Ÿæˆ
â”‚       â”œâ”€â”€ base.py        # ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ç”Ÿæˆå™¨
â”‚       â”œâ”€â”€ numeric.py     # æ•°å€¤ç‰¹å¾´é‡
â”‚       â”œâ”€â”€ categorical.py # ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡
â”‚       â”œâ”€â”€ interaction.py # äº¤äº’ä½œç”¨ç‰¹å¾´é‡
â”‚       â”œâ”€â”€ datetime.py    # æ—¥æ™‚ç‰¹å¾´é‡
â”‚       â”œâ”€â”€ aggregation.py # é›†ç´„ç‰¹å¾´é‡
â”‚       â””â”€â”€ pipeline.py    # ç‰¹å¾´é‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
â”œâ”€â”€ modeling/              # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
â”‚   â”œâ”€â”€ base.py           # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹
â”‚   â”œâ”€â”€ tree_models.py    # ãƒ„ãƒªãƒ¼ãƒ¢ãƒ‡ãƒ«ï¼ˆXGBoost, LightGBM, CatBoostï¼‰
â”‚   â”œâ”€â”€ linear_models.py  # ç·šå½¢ãƒ¢ãƒ‡ãƒ«
â”‚   â”œâ”€â”€ ensemble.py       # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•
â”‚   â””â”€â”€ factory.py        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒª
â”œâ”€â”€ optimization/          # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
â”‚   â”œâ”€â”€ base.py           # ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ã‚¯ãƒ©ã‚¹
â”‚   â”œâ”€â”€ grid_search.py    # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ
â”‚   â”œâ”€â”€ random_search.py  # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ¼ãƒ
â”‚   â”œâ”€â”€ bayesian_optimization.py # ãƒ™ã‚¤ã‚¸ã‚¢ãƒ³æœ€é©åŒ–
â”‚   â”œâ”€â”€ optuna_optimizer.py # Optunaæœ€é©åŒ–
â”‚   â””â”€â”€ factory.py        # æœ€é©åŒ–ãƒ•ã‚¡ã‚¯ãƒˆãƒª
â”œâ”€â”€ evaluation/           # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
â”‚   â”œâ”€â”€ metrics.py        # è©•ä¾¡æŒ‡æ¨™
â”‚   â””â”€â”€ validation.py     # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”œâ”€â”€ submission/           # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
â”‚   â””â”€â”€ submission_generator.py
â”œâ”€â”€ config/               # è¨­å®šç®¡ç†
â”‚   â””â”€â”€ kaggle_config.py  # Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³è¨­å®š
â””â”€â”€ utils/                # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    â”œâ”€â”€ base.py           # ãƒ™ãƒ¼ã‚¹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    â”œâ”€â”€ config.py         # è¨­å®šç®¡ç†
    â””â”€â”€ io.py             # ãƒ•ã‚¡ã‚¤ãƒ«å…¥å‡ºåŠ›æ“ä½œ

notebooks/                # Jupyter ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯
â”œâ”€â”€ kaggle_submission_notebook.ipynb  # çµ±åˆæå‡ºãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯
â”œâ”€â”€ 01_data_exploration_preprocessing.ipynb
â”œâ”€â”€ 02_model_training_evaluation.ipynb
â”œâ”€â”€ 03_ensemble_hyperparameter_tuning.ipynb
â””â”€â”€ 04_results_analysis_feature_importance.ipynb
```

## ä½¿ç”¨ä¾‹

### 1. ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```python
from src.preprocessing.preprocessor import DataPreprocessor
from src.data.data_loader import DataLoader

# ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
loader = DataLoader()
train_data, test_data = loader.load_train_test()

# ã‚«ã‚¹ã‚¿ãƒ å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
preprocessor = DataPreprocessor()
X_train = train_data.drop(['Personality', 'id'], axis=1)
y_train = train_data['Personality']

X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(test_data.drop('id', axis=1))
```

### 2. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¨è©•ä¾¡

```python
from src.modeling.factory import create_kaggle_models
from src.evaluation.validation import CompetitionValidator

# ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
models = create_kaggle_models(target_type='regression')

# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡
validator = CompetitionValidator()
cv_results = {}

for model in models:
    cv_result = validator.cross_validate_model(model, X_train_processed, y_train, cv=5)
    cv_results[model.config.name] = cv_result
    print(f"{model.config.name}: CV RMSE = {cv_result['mean_rmse']:.4f}")
```

### 3. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•

```python
from src.modeling.ensemble import create_ensemble_from_models, create_optimized_ensemble

# å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
trained_models = {}
for model in models:
    model.fit(X_train_processed, y_train)
    trained_models[model.config.name] = model

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®ä½œæˆ
ensemble = create_ensemble_from_models(list(trained_models.values()), 'average')
ensemble.fit(X_train_processed, y_train)

# äºˆæ¸¬ã®ç”Ÿæˆ
predictions = ensemble.predict(X_test_processed)
```

### 4. åˆ†æã¨ç‰¹å¾´é‡é‡è¦åº¦

```python
from src.analysis.feature_importance import FeatureImportanceAnalyzer

# ç‰¹å¾´é‡é‡è¦åº¦ã®åˆ†æ
analyzer = FeatureImportanceAnalyzer()
importance_results = analyzer.analyze_models(trained_models, X_train_processed, y_train)

# åŒ…æ‹¬çš„åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
from src.analysis.comprehensive_analysis import ComprehensiveAnalysis

analysis = ComprehensiveAnalysis()
report = analysis.generate_report(trained_models, X_train_processed, y_train, X_test_processed)
```

## è¨­å®š

```python
from src.config.kaggle_config import KaggleConfig, ConfigPresets

# å›å¸°ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ç”¨è¨­å®š
config = ConfigPresets.regression_competition()

# åˆ†é¡ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ç”¨è¨­å®š
config = ConfigPresets.classification_competition()

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
config = KaggleConfig(
    problem_type='regression',
    preprocessing={
        'handle_missing': True,
        'handle_outliers': True,
        'outlier_threshold': 2.0
    },
    feature_engineering={
        'numeric_features': True,
        'polynomial_features': True,
        'max_interactions': 150
    }
)
```

## ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

```bash
# å…¨ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
make test

# é«˜é€Ÿãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œï¼ˆã‚¹ãƒ­ãƒ¼ãƒãƒ¼ã‚«ãƒ¼ã‚’é™¤å¤–ï¼‰
make test-fast

# ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã®ã¿ã®å®Ÿè¡Œ
make test-unit

# çµ±åˆãƒ†ã‚¹ãƒˆã®ã¿ã®å®Ÿè¡Œ
make test-integration

# ã‚«ãƒãƒ¬ãƒƒã‚¸ä»˜ããƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
make test-coverage

# ã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
make test-smoke
```

## é–‹ç™ºã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

### ã‚³ãƒ¼ãƒ‰å“è³ª
- **å‹ãƒ’ãƒ³ãƒˆ**: ã™ã¹ã¦ã®é–¢æ•°ã¨ãƒ¡ã‚½ãƒƒãƒ‰ã«å‹ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
- **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: ä¸»è¦ã‚¯ãƒ©ã‚¹ã¨é–¢æ•°ã«Googleã‚¹ã‚¿ã‚¤ãƒ«ã®docstring
- **ãƒ†ã‚¹ãƒˆ**: pytestã‚’ä½¿ç”¨ã—ãŸãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
- **ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: blackã«ã‚ˆã‚‹è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- **CPUæœ€é©åŒ–**: GPUä¸è¦ã®ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®é«˜é€Ÿå®Ÿè¡Œ
- **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åŠ¹ç‡çš„ãªå‡¦ç†
- **ä¸¦åˆ—å‡¦ç†**: é©ç”¨å¯èƒ½ãªç®‡æ‰€ã§ã®ä¸¦åˆ—åŒ–

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
- **æ©Ÿå¯†æƒ…å ±**: APIã‚­ãƒ¼ã‚„èªè¨¼æƒ…å ±ã®ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ç¦æ­¢
- **å…¥åŠ›æ¤œè¨¼**: å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®é©åˆ‡ãªæ¤œè¨¼ã¨ã‚µãƒ‹ã‚¿ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³

## åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰

```bash
# ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
make install              # åŸºæœ¬ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
make dev-install         # é–‹ç™ºãƒ„ãƒ¼ãƒ«ä»˜ãã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
make setup               # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ

# ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¨ãƒ¢ãƒ‡ãƒªãƒ³ã‚°
make preprocess          # å‰å‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ†ã‚¹ãƒˆ
make model-xgb           # XGBoostãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ
make model-lgb           # LightGBMãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ
make ensemble-stacking   # ã‚¹ã‚¿ãƒƒã‚­ãƒ³ã‚°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®ãƒ†ã‚¹ãƒˆ

# ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å®Ÿè¡Œ
make notebook-run        # Kaggleæå‡ºãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å®Ÿè¡Œ
make notebook-clean      # ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å‡ºåŠ›ã®ã‚¯ãƒªãƒ¼ãƒ³

# Kaggleãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
make personality-prediction
make personality-prediction  # æ€§æ ¼äºˆæ¸¬ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å®Ÿè¡Œ

# é–‹ç™ºã¨ãƒ†ã‚¹ãƒˆ
make test               # ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
make lint               # ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯
make format             # ã‚³ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
make clean              # ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³
```

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License

## è²¢çŒ®

1. ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ•ã‚©ãƒ¼ã‚¯
2. æ©Ÿèƒ½ãƒ–ãƒ©ãƒ³ãƒã‚’ä½œæˆ (`git checkout -b feature/amazing-feature`)
3. å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆ (`git commit -m 'Add amazing feature'`)
4. ãƒ–ãƒ©ãƒ³ãƒã«ãƒ—ãƒƒã‚·ãƒ¥ (`git push origin feature/amazing-feature`)
5. Pull Requestã‚’ä½œæˆ

## ã‚µãƒãƒ¼ãƒˆ

- Issues: GitHub Issues ã§ãƒã‚°å ±å‘Šãƒ»è³ªå•
- Discussions: GitHub Discussions ã§ä¸€èˆ¬çš„ãªè­°è«–